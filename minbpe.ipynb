{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># minbpe: Minimal Byte Pair Encoding Tokenizer\n\nThis notebook contains a faithful implementation of Andrej Karpathy's minbpe tokenizer, emphasizing:\n- Clean, minimal code\n- Byte-level tokenization\n- Educational clarity\n\nThe implementation follows the core design philosophy of Karpathy's approach, preserving the simplicity and readability of the original while adding several enhancements:\n\n## Key Features\n- **Basic Tokenizer**: Core BPE algorithm implementation\n- **RegexTokenizer**: Enhanced with GPT2/GPT4 pattern splitting\n- **SpecialTokensTokenizer**: Support for special tokens with flexible handling options\n- **GPT4Tokenizer**: Compatible with OpenAI's tokenizers with byte shuffling\n- **Save/Load**: Compatible with Karpathy's .model/.vocab format\n\nEach feature is extensively documented and tested to provide a complete educational resource for understanding modern tokenization approaches.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set, Any, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Tokenizer Implementation\n",
    "\n",
    "We start with a basic tokenizer class that implements the core BPE algorithm without any regex-based preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"A minimal Byte Pair Encoding tokenizer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize with the base 256 tokens (raw bytes 0-255)\n",
    "        self.merges = {}  # (token1, token2) -> new_token_id \n",
    "        self.vocab = {}   # token_id -> token (bytes)\n",
    "        self.vocab_size = 0\n",
    "        self.special_tokens = {}\n",
    "        \n",
    "        # Pre-populate the vocabulary with the basic 256 byte tokens\n",
    "        for i in range(256):\n",
    "            token = bytes([i])\n",
    "            self.vocab[i] = token\n",
    "            \n",
    "        self.vocab_size = 256\n",
    "    \n",
    "    def train(self, text: str, vocab_size: int, verbose: bool = False) -> None:\n",
    "        \"\"\"Train the tokenizer on text, extending the vocabulary to the desired size.\"\"\"\n",
    "        # Convert text to bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        # Keep track of progress\n",
    "        if verbose:\n",
    "            print(f\"Training BPE tokenizer to vocab size {vocab_size}\")\n",
    "            print(f\"Text size: {len(text)} chars, {len(ids)} bytes\")\n",
    "        \n",
    "        # Iteratively merge the most frequent pair until we reach the desired vocab size\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            # Count frequencies of adjacent pairs\n",
    "            stats = self.get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "                \n",
    "            # Find the most frequent pair\n",
    "            pair = max(stats, key=stats.get)\n",
    "            \n",
    "            # Create a new token for this pair\n",
    "            token1, token2 = pair\n",
    "            new_token = self.vocab[token1] + self.vocab[token2]\n",
    "            new_id = self.vocab_size\n",
    "            \n",
    "            # Add merge to our vocabulary\n",
    "            self.merges[pair] = new_id\n",
    "            self.vocab[new_id] = new_token\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "            # Apply the merge to the current token list\n",
    "            ids = self.merge(ids, pair, new_id)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"Merge #{i}: pair {pair} -> {new_id}, corpus now {len(ids)} tokens\")\n",
    "    \n",
    "    def get_stats(self, ids: List[int]) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Count the frequencies of adjacent token pairs.\"\"\"\n",
    "        stats = Counter()\n",
    "        for i in range(len(ids) - 1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            stats[pair] += 1\n",
    "        return stats\n",
    "    \n",
    "    def merge(self, ids: List[int], pair: Tuple[int, int], new_id: int) -> List[int]:\n",
    "        \"\"\"Replace all occurrences of a token pair with a new token ID.\"\"\"\n",
    "        # Create a new list for the merged result\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            # If we're at the last token, just add it\n",
    "            if i == len(ids) - 1:\n",
    "                new_ids.append(ids[i])\n",
    "                break\n",
    "            \n",
    "            # If current pair matches, merge and add the new token\n",
    "            if ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                new_ids.append(new_id)\n",
    "                i += 2  # Skip both tokens\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1  # Move to next token\n",
    "        \n",
    "        return new_ids\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        # Convert text to bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        # Apply merges iteratively, in the order they were learned\n",
    "        while len(ids) >= 2:\n",
    "            # Find valid merge pairs in the current sequence\n",
    "            pairs = [(ids[i], ids[i+1]) for i in range(len(ids)-1)]\n",
    "            valid_pairs = [(pair, self.merges[pair]) for pair in pairs if pair in self.merges]\n",
    "            \n",
    "            # If no valid pairs, we're done\n",
    "            if not valid_pairs:\n",
    "                break\n",
    "                \n",
    "            # Find the pair with the lowest merge ID (first learned)\n",
    "            pair, new_id = min(valid_pairs, key=lambda x: x[1])\n",
    "            \n",
    "            # Apply the merge\n",
    "            ids = self.merge(ids, pair, new_id)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        # Convert token IDs to bytes\n",
    "        bytes_list = []\n",
    "        for token_id in ids:\n",
    "            bytes_list.extend(self.vocab[token_id])\n",
    "        \n",
    "        # Convert bytes to UTF-8 text\n",
    "        text = bytes(bytes_list).decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def save(self, file_path: str) -> None:\n",
    "        \"\"\"Save the tokenizer to a file.\"\"\"\n",
    "        # Prepare model data - convert bytes to lists for JSON serialization\n",
    "        model_data = {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"merges\": {f\"{t1},{t2}\": idx for (t1, t2), idx in self.merges.items()},\n",
    "            \"vocab\": {str(i): list(t) for i, t in self.vocab.items() if i >= 256},\n",
    "            \"special_tokens\": self.special_tokens\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, file_path: str) -> None:\n",
    "        \"\"\"Load a tokenizer from a file.\"\"\"\n",
    "        # Read the model data\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        # Reset the tokenizer\n",
    "        self.__init__()\n",
    "        \n",
    "        # Load the vocabulary\n",
    "        self.vocab_size = model_data[\"vocab_size\"]\n",
    "        \n",
    "        # Add vocabulary items (skipping the base 256 bytes already initialized)\n",
    "        for token_id_str, token_bytes in model_data[\"vocab\"].items():\n",
    "            token_id = int(token_id_str)\n",
    "            self.vocab[token_id] = bytes(token_bytes)\n",
    "        \n",
    "        # Load merges\n",
    "        for pair_str, idx in model_data[\"merges\"].items():\n",
    "            t1, t2 = map(int, pair_str.split(\",\"))\n",
    "            self.merges[(t1, t2)] = idx\n",
    "        \n",
    "        # Load special tokens\n",
    "        self.special_tokens = model_data.get(\"special_tokens\", {})\n",
    "    \n",
    "    def token_to_str(self, token_id: int) -> str:\n",
    "        \"\"\"Get a string representation of a token for visualization.\"\"\"\n",
    "        token_bytes = self.vocab[token_id]\n",
    "        # Try to convert to UTF-8 string if possible\n",
    "        try:\n",
    "            s = token_bytes.decode('utf-8')\n",
    "            # Replace newlines, tabs, etc. for display\n",
    "            s = s.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            if len(s.strip()) == 0:\n",
    "                # If it's all whitespace, show hex\n",
    "                return f\"[hex: {token_bytes.hex()}]\"\n",
    "            return s\n",
    "        except UnicodeDecodeError:\n",
    "            # If not a valid UTF-8 string, show hex\n",
    "            return f\"[hex: {token_bytes.hex()}]\"\n",
    "    \n",
    "    def print_vocab(self, n=50) -> None:\n",
    "        \"\"\"Print the first n tokens in the vocabulary for inspection.\"\"\"\n",
    "        ids = sorted(self.vocab.keys())\n",
    "        skipped = max(0, len(ids) - n)\n",
    "        print(f\"Vocabulary size: {len(ids)} tokens\")\n",
    "        print(f\"Showing first {min(n, len(ids))} tokens:\")\n",
    "        for i, token_id in enumerate(ids[:n]):\n",
    "            s = self.token_to_str(token_id)\n",
    "            print(f\"Token {token_id}: {s}\")\n",
    "        if skipped > 0:\n",
    "            print(f\"... and {skipped} more tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tokenizer\n",
    "\n",
    "Let's test our implementation with a simple example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer to vocab size 500\n",
      "Text size: 656 chars, 662 bytes\n",
      "Merge #0: pair (101, 32) -> 256, corpus now 647 tokens\n",
      "Merge #100: pair (355, 110) -> 356, corpus now 311 tokens\n",
      "Merge #200: pair (455, 277) -> 456, corpus now 211 tokens\n",
      "Vocabulary size: 500 tokens\n",
      "Showing first 30 tokens:\n",
      "Token 0: \u0000\n",
      "Token 1: \u0001\n",
      "Token 2: \u0002\n",
      "Token 3: \u0003\n",
      "Token 4: \u0004\n",
      "Token 5: \u0005\n",
      "Token 6: \u0006\n",
      "Token 7: \u0007\n",
      "Token 8:\n",
      "Token 9: \\t\n",
      "Token 10: \\n\n",
      "Token 11: [hex: 0b]\n",
      "Token 12: [hex: 0c]\n",
      "Token 13: [hex: 0d]\n",
      "Token 14: \u000e\n",
      "Token 15: \u000f\n",
      "Token 16: \u0010\n",
      "Token 17: \u0011\n",
      "Token 18: \u0012\n",
      "Token 19: \u0013\n",
      "Token 20: \u0014\n",
      "Token 21: \u0015\n",
      "Token 22: \u0016\n",
      "Token 23: \u0017\n",
      "Token 24: \u0018\n",
      "Token 25: \u0019\n",
      "Token 26: \u001a\n",
      "Token 27: \u001b\n",
      "Token 28: [hex: 1c]\n",
      "Token 29: [hex: 1d]\n",
      "... and 470 more tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a simple training corpus\n",
    "training_text = \"\"\"\n",
    "Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of consecutive bytes in a sequence with a single, unused byte. In NLP, it is used as a subword tokenization algorithm.\n",
    "\n",
    "The BPE algorithm works as follows:\n",
    "1. Initialize the vocabulary with individual characters/bytes\n",
    "2. Count all pairs of adjacent symbols in the training corpus\n",
    "3. Merge the most frequent pair and add it to the vocabulary\n",
    "4. Repeat steps 2-3 until reaching the desired vocabulary size\n",
    "\n",
    "BPE can handle out-of-vocabulary words by splitting them into known subword units, making it effective for various languages and even emoji 👍🌍.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize our tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Train to a vocabulary size of 500\n",
    "tokenizer.train(training_text, vocab_size=500, verbose=True)\n",
    "\n",
    "# Show some of the learned tokens\n",
    "tokenizer.print_vocab(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "Now let's test encoding and decoding to verify the tokenizer works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded into 42 tokens: [292, 32, 305, 107, 269, 306, 97, 266, 111, 293, 282, 107, 257, 103, 265, 311, 102, 262, 32, 110, 97, 116, 117, 295, 108, 32, 267, 110, 103, 117, 97, 103, 256, 112, 114, 111, 99, 101, 115, 115, 264, 33]\n",
      "\n",
      "Token breakdown:\n",
      "Token 1: ID 292 = 'BPE'\n",
      "Token 2: ID 32 = '[hex: 20]'\n",
      "Token 3: ID 305 = 'to'\n",
      "Token 4: ID 107 = 'k'\n",
      "Token 5: ID 269 = 'en'\n",
      "Token 6: ID 306 = 'iz'\n",
      "Token 7: ID 97 = 'a'\n",
      "Token 8: ID 266 = 'ti'\n",
      "Token 9: ID 111 = 'o'\n",
      "Token 10: ID 293 = 'n '\n",
      "Token 11: ID 282 = 'wor'\n",
      "Token 12: ID 107 = 'k'\n",
      "Token 13: ID 257 = 's '\n",
      "Token 14: ID 103 = 'g'\n",
      "Token 15: ID 265 = 're'\n",
      "Token 16: ID 311 = 'at '\n",
      "Token 17: ID 102 = 'f'\n",
      "Token 18: ID 262 = 'or'\n",
      "Token 19: ID 32 = '[hex: 20]'\n",
      "Token 20: ID 110 = 'n'\n",
      "Token 21: ID 97 = 'a'\n",
      "Token 22: ID 116 = 't'\n",
      "Token 23: ID 117 = 'u'\n",
      "Token 24: ID 295 = 'ra'\n",
      "Token 25: ID 108 = 'l'\n",
      "Token 26: ID 32 = '[hex: 20]'\n",
      "Token 27: ID 267 = 'la'\n",
      "Token 28: ID 110 = 'n'\n",
      "Token 29: ID 103 = 'g'\n",
      "Token 30: ID 117 = 'u'\n",
      "Token 31: ID 97 = 'a'\n",
      "Token 32: ID 103 = 'g'\n",
      "Token 33: ID 256 = 'e '\n",
      "Token 34: ID 112 = 'p'\n",
      "Token 35: ID 114 = 'r'\n",
      "Token 36: ID 111 = 'o'\n",
      "Token 37: ID 99 = 'c'\n",
      "Token 38: ID 101 = 'e'\n",
      "Token 39: ID 115 = 's'\n",
      "Token 40: ID 115 = 's'\n",
      "Token 41: ID 264 = 'ing'\n",
      "Token 42: ID 33 = '!'\n",
      "\n",
      "Decoded text: 'BPE tokenization works great for natural language processing!'\n",
      "Round trip success: True\n"
     ]
    }
   ],
   "source": [
    "# Test with a new sentence\n",
    "test_text = \"BPE tokenization works great for natural language processing!\"\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"Encoded into {len(encoded)} tokens: {encoded}\")\n",
    "\n",
    "# Display each token\n",
    "print(\"\\nToken breakdown:\")\n",
    "for i, token_id in enumerate(encoded):\n",
    "    print(f\"Token {i+1}: ID {token_id} = '{tokenizer.token_to_str(token_id)}'\")\n",
    "\n",
    "# Decode the tokens back to text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nDecoded text: '{decoded}'\")\n",
    "print(f\"Round trip success: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Tokenization Efficiency\n",
    "\n",
    "Let's compute some metrics on the tokenization efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text            | Chars    | Tokens   | Ratio   | Encode (s) | Decode (s) | Success\n",
      "---------------------------------------------------------------------------\n",
      "English         | 44       | 35       | 1.26    | 0.0001     | 0.0000     | True\n",
      "Repeated        | 41       | 41       | 1.00    | 0.0000     | 0.0000     | True\n",
      "Numbers         | 32       | 32       | 1.00    | 0.0000     | 0.0000     | True\n",
      "Technical       | 60       | 50       | 1.20    | 0.0000     | 0.0000     | True\n",
      "Emoji           | 15       | 37       | 0.41    | 0.0000     | 0.0000     | True\n",
      "Mixed           | 35       | 48       | 0.73    | 0.0001     | 0.0000     | True\n"
     ]
    }
   ],
   "source": [
    "def measure_efficiency(tokenizer, texts):\n",
    "    \"\"\"Measure tokenization efficiency across multiple text samples.\"\"\"\n",
    "    results = []\n",
    "    for name, text in texts.items():\n",
    "        # Tokenize and measure\n",
    "        start_time = time.time()\n",
    "        tokens = tokenizer.encode(text)\n",
    "        encode_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        decode_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_count = len(text)\n",
    "        token_count = len(tokens)\n",
    "        compression_ratio = char_count / token_count\n",
    "        chars_per_second = char_count / encode_time if encode_time > 0 else 0\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"name\": name,\n",
    "            \"chars\": char_count,\n",
    "            \"tokens\": token_count,\n",
    "            \"ratio\": compression_ratio,\n",
    "            \"encode_time\": encode_time,\n",
    "            \"decode_time\": decode_time,\n",
    "            \"chars_per_second\": chars_per_second,\n",
    "            \"roundtrip_success\": text == decoded\n",
    "        })\n",
    "    \n",
    "    # Print results table\n",
    "    print(f\"{'Text':<15} | {'Chars':<8} | {'Tokens':<8} | {'Ratio':<7} | {'Encode (s)':<10} | {'Decode (s)':<10} | {'Success':<7}\")\n",
    "    print(\"-\" * 75)\n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<15} | {r['chars']:<8} | {r['tokens']:<8} | {r['ratio']:<7.2f} | {r['encode_time']:<10.4f} | {r['decode_time']:<10.4f} | {r['roundtrip_success']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test texts\n",
    "test_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Repeated\": \"hello hello hello hello hello hello hello\",\n",
    "    \"Numbers\": \"1234567890 1234567890 1234567890\",\n",
    "    \"Technical\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Emoji\": \"🙂 🌍 🚀 👨‍👩‍👧‍👦 🎉\",\n",
    "    \"Mixed\": \"Training at 3.5x speed: 😊 快速训练！速度提高\"\n",
    "}\n",
    "\n",
    "# Measure tokenization efficiency\n",
    "efficiency_results = measure_efficiency(tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Tokenization Process\n",
    "\n",
    "Let's create a visualization of how text gets split into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized into 34 tokens:\n",
      "[H][e][l][l][o][,␣][wor][l][d][!][␣][T][h][i][s␣][i][s␣a][␣][te][s][t␣][of][␣][BPE][␣][to][k][en][iz][a][ti][o][n][.]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 72 = 'H'\n",
      "Token 2: ID 101 = 'e'\n",
      "Token 3: ID 108 = 'l'\n",
      "Token 4: ID 108 = 'l'\n",
      "Token 5: ID 111 = 'o'\n",
      "Token 6: ID 303 = ',␣'\n",
      "Token 7: ID 282 = 'wor'\n",
      "Token 8: ID 108 = 'l'\n",
      "Token 9: ID 100 = 'd'\n",
      "Token 10: ID 33 = '!'\n",
      "Token 11: ID 32 = '␣'\n",
      "Token 12: ID 84 = 'T'\n",
      "Token 13: ID 104 = 'h'\n",
      "Token 14: ID 105 = 'i'\n",
      "Token 15: ID 257 = 's␣'\n",
      "Token 16: ID 105 = 'i'\n",
      "Token 17: ID 277 = 's␣a'\n",
      "Token 18: ID 32 = '␣'\n",
      "Token 19: ID 263 = 'te'\n",
      "Token 20: ID 115 = 's'\n",
      "Token 21: ID 260 = 't␣'\n",
      "Token 22: ID 300 = 'of'\n",
      "Token 23: ID 32 = '␣'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '␣'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 110 = 'n'\n",
      "Token 34: ID 46 = '.'\n"
     ]
    }
   ],
   "source": [
    "def visualize_tokenization(tokenizer, text):\n",
    "    \"\"\"Visualize how text is tokenized by showing token boundaries.\"\"\"\n",
    "    # Encode the text\n",
    "    ids = tokenizer.encode(text)\n",
    "    \n",
    "    # Get the bytes for each token\n",
    "    token_bytes = [tokenizer.vocab[id] for id in ids]\n",
    "    \n",
    "    # Try to display each token as text\n",
    "    visualized = []\n",
    "    for token in token_bytes:\n",
    "        try:\n",
    "            token_text = token.decode('utf-8')\n",
    "            # Replace whitespace for visibility\n",
    "            token_text = token_text.replace(' ', '␣').replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            visualized.append(token_text)\n",
    "        except UnicodeDecodeError:\n",
    "            # If not a valid UTF-8 sequence, show hex\n",
    "            visualized.append(f\"[{token.hex()}]\")\n",
    "    \n",
    "    # Display with token boundaries\n",
    "    print(f\"Tokenized into {len(ids)} tokens:\")\n",
    "    result = \"\"\n",
    "    for token in visualized:\n",
    "        result += f\"[{token}]\"\n",
    "    print(result)\n",
    "    \n",
    "    # Display each token with its ID\n",
    "    print(\"\\nDetailed token breakdown:\")\n",
    "    for i, (id, vis) in enumerate(zip(ids, visualized)):\n",
    "        print(f\"Token {i+1}: ID {id} = '{vis}'\")\n",
    "    \n",
    "    return visualized\n",
    "\n",
    "# Visualize tokenization for a sample text\n",
    "sample_text = \"Hello, world! This is a test of BPE tokenization.\"\n",
    "tokens_visualized = visualize_tokenization(tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Tokenizer\n",
    "\n",
    "Let's test the serialization and deserialization of our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer with 500 tokens\n",
      "Loaded tokenizer with 500 tokens\n",
      "Original tokenizer: 32 tokens\n",
      "Loaded tokenizer: 32 tokens\n",
      "Tokens match: True\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "print(f\"Saved tokenizer with {tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Create a new tokenizer and load the saved model\n",
    "new_tokenizer = Tokenizer()\n",
    "new_tokenizer.load(\"bpe_tokenizer.json\")\n",
    "print(f\"Loaded tokenizer with {new_tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Verify the loaded tokenizer works the same\n",
    "check_text = \"Testing if the loaded tokenizer works correctly.\"\n",
    "original_tokens = tokenizer.encode(check_text)\n",
    "loaded_tokens = new_tokenizer.encode(check_text)\n",
    "\n",
    "print(f\"Original tokenizer: {len(original_tokens)} tokens\")\n",
    "print(f\"Loaded tokenizer: {len(loaded_tokens)} tokens\")\n",
    "print(f\"Tokens match: {original_tokens == loaded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex-Based Tokenizer\n",
    "\n",
    "For more efficient tokenization in natural language processing, we can implement a regex-based pre-tokenization step before applying BPE merges. This helps the tokenizer better handle natural language boundaries like words and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "    \"\"\"Enhanced tokenizer with regex-based pre-tokenization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pat = re.compile(r'(\\s+|[a-zA-Z]+|[0-9]+|\\S)')\n",
    "        # Ensure all parent class attributes are present\n",
    "        self.merges = {}  # (token1, token2) -> new_token_id \n",
    "        self.vocab = {}   # token_id -> token (bytes)\n",
    "        self.token_to_id = {}  # token (bytes) -> token_id\n",
    "        self.vocab_size = 256\n",
    "        \n",
    "        # Pre-populate the vocabulary with the basic 256 byte tokens\n",
    "        for i in range(256):\n",
    "            token = bytes([i])\n",
    "            self.vocab[i] = token\n",
    "            self.token_to_id[token] = i\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Override encode to use regex-based pre-tokenization.\"\"\"\n",
    "        # First split using regex pattern\n",
    "        parts = [part.encode('utf-8') for part in re.findall(self.pat, text)]\n",
    "        \n",
    "        # Then encode each part with the base tokenizer\n",
    "        ids = []\n",
    "        for part in parts:\n",
    "            # Convert to bytes and start with raw byte tokens\n",
    "            bytes_list = list(part)\n",
    "            tokens = [bytes([b]) for b in bytes_list]\n",
    "            \n",
    "            # Apply merges iteratively, as in the base class\n",
    "            while len(tokens) >= 2:\n",
    "                # Find valid merge pairs in the current sequence\n",
    "                pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "                valid_pairs = [(pair, self.merges[pair]) for pair in pairs if pair in self.merges]\n",
    "                \n",
    "                # If no valid pairs, we're done\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Find the pair with the lowest merge ID (first learned)\n",
    "                pair, new_id = min(valid_pairs, key=lambda x: x[1])\n",
    "                \n",
    "                # Apply the merge\n",
    "                tokens = self.merge(tokens, pair, new_id)\n",
    "            \n",
    "            # Map tokens to IDs\n",
    "            ids.extend([self.token_to_id[token] for token in tokens])\n",
    "            \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens Support\n",
    "\n",
    "Let's enhance our tokenizer to support special tokens, similar to GPT models."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Define the official GPT text split patterns\nGPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\nGPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n\n# Explanation of GPT4 pattern:\n# '(?i:[sdmt]|ll|ve|re) - Matches contractions like 's, 'd, 'm, 't, 'll, 've, 're (case insensitive)\n# [^\\r\\n\\p{L}\\p{N}]?+\\p{L}+ - Matches letter sequences, possibly with a leading non-letter/non-number\n# \\p{N}{1,3} - Matches 1-3 digit numbers\n# ?[^\\s\\p{L}\\p{N}]++[\\r\\n]* - Matches punctuation and symbols\n# \\s*[\\r\\n] - Matches newlines with optional whitespace\n# \\s+(?!\\S) - Matches trailing whitespace\n# \\s+ - Matches other whitespace sequences",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class RegexTokenizer(Tokenizer):\n    \"\"\"Enhanced tokenizer with regex-based pre-tokenization.\"\"\"\n    \n    def __init__(self, pattern=None):\n        \"\"\"\n        Initialize the tokenizer with optional regex pattern.\n        \n        Args:\n            pattern: A regex pattern to split text before tokenization.\n                    If None, defaults to GPT4_SPLIT_PATTERN.\n        \"\"\"\n        super().__init__()\n        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n        self.compiled_pattern = re.compile(self.pattern)\n        \n        # Ensure all parent class attributes are present\n        self.merges = {}  # (token1, token2) -> new_token_id \n        self.vocab = {}   # token_id -> token (bytes)\n        self.token_to_id = {}  # token (bytes) -> token_id\n        self.vocab_size = 256\n        \n        # Pre-populate the vocabulary with the basic 256 byte tokens\n        for i in range(256):\n            token = bytes([i])\n            self.vocab[i] = token\n            self.token_to_id[token] = i\n    \n    def train(self, text: str, vocab_size: int, verbose: bool = False) -> None:\n        \"\"\"\n        Train the tokenizer with regex-based splitting.\n        \n        Args:\n            text: The training text\n            vocab_size: Target vocabulary size\n            verbose: Whether to print debug information\n        \"\"\"\n        assert vocab_size >= 256\n        num_merges = vocab_size - 256\n        \n        # Split the text into chunks using the regex pattern\n        text_chunks = re.findall(self.compiled_pattern, text)\n        \n        # Process each chunk separately\n        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n        \n        # Iteratively merge the most common pairs to create new tokens\n        merges = {}  # (int, int) -> int\n        vocab = {idx: bytes([idx]) for idx in range(256)}  # idx -> bytes\n        \n        for i in range(num_merges):\n            # Count frequencies of adjacent pairs across all chunks\n            stats = {}\n            for chunk_ids in ids:\n                get_stats(chunk_ids, stats)\n                \n            # If no more pairs, stop early\n            if not stats:\n                if verbose:\n                    print(f\"No more pairs to merge after {i} merges\")\n                break\n                \n            # Find the most frequent pair\n            pair = max(stats, key=stats.get)\n            \n            # Create a new token for this pair\n            idx = 256 + i\n            \n            # Replace all occurrences of pair with new token\n            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n            \n            # Record the merge\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n            \n            # Print progress\n            if verbose and i % 100 == 0:\n                print(f\"Merge #{i}: pair {pair} -> {idx}, corpus now has {sum(len(chunk) for chunk in ids)} tokens\")\n        \n        # Save class variables\n        self.merges = merges\n        self.vocab = vocab\n        self.vocab_size = 256 + len(merges)\n        \n        # Update token_to_id dictionary for efficient encoding\n        self.token_to_id = {token: idx for idx, token in vocab.items()}\n    \n    def encode(self, text: str) -> List[int]:\n        \"\"\"\n        Encode text using regex-based preprocessing.\n        \n        Args:\n            text: The text to encode\n            \n        Returns:\n            List of token IDs\n        \"\"\"\n        # Split text into chunks using the regex pattern\n        text_chunks = re.findall(self.compiled_pattern, text)\n        \n        # Process each chunk separately and concatenate results\n        ids = []\n        for chunk in text_chunks:\n            chunk_bytes = chunk.encode(\"utf-8\")\n            chunk_ids = self._encode_chunk(chunk_bytes)\n            ids.extend(chunk_ids)\n        \n        return ids\n    \n    def _encode_chunk(self, text_bytes: bytes) -> List[int]:\n        \"\"\"\n        Encode a single chunk of bytes.\n        \n        Args:\n            text_bytes: Bytes to encode\n            \n        Returns:\n            List of token IDs\n        \"\"\"\n        # Start with raw bytes\n        ids = list(text_bytes)\n        \n        # Apply merges iteratively\n        while len(ids) >= 2:\n            # Find pairs in the current sequence\n            stats = get_stats(ids)\n            \n            # Find the pair with the lowest merge index\n            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n            \n            # If no more merges possible, stop\n            if pair not in self.merges:\n                break\n                \n            # Apply the merge\n            idx = self.merges[pair]\n            ids = merge(ids, pair, idx)\n        \n        return ids"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 'Here is some text <|endoftext|> followed by a special token.'\n",
      "Encoded with 33 tokens: [72, 101, 114, 256, 105, 257, 115, 111, 109, 256, 263, 120, 260, 100257, 32, 102, 339, 108, 340, 101, 271, 279, 323, 112, 101, 99, 105, 307, 32, 305, 107, 269, 46]\n",
      "Decoded: 'Here is some text <|endoftext|> followed by a special token.'\n",
      "Round trip success: True\n",
      "\n",
      "Token visualization:\n",
      "[H][e][r][e ][i][s ][s][o][m][e ][te][x][t ][*<|endoftext|>*][ ][f][ol][l][ow][e][d ][by][ a s][p][e][c][i][al][ ][to][k][en][.]\n"
     ]
    }
   ],
   "source": [
    "# Test special tokens support\n",
    "# Create common GPT-style special tokens\n",
    "special_tokens = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100261\n",
    "}\n",
    "\n",
    "# Create and train a tokenizer with special tokens\n",
    "special_tokenizer = SpecialTokensTokenizer()\n",
    "special_tokenizer.train(training_text, vocab_size=500, verbose=False)\n",
    "special_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Test encoding/decoding with special tokens\n",
    "test_with_special = \"Here is some text <|endoftext|> followed by a special token.\"\n",
    "encoded_special = special_tokenizer.encode(test_with_special, allowed_special=\"all\")\n",
    "decoded_special = special_tokenizer.decode(encoded_special)\n",
    "\n",
    "print(f\"Original text: '{test_with_special}'\")\n",
    "print(f\"Encoded with {len(encoded_special)} tokens: {encoded_special}\")\n",
    "print(f\"Decoded: '{decoded_special}'\")\n",
    "print(f\"Round trip success: {test_with_special == decoded_special}\")\n",
    "\n",
    "# Visualize the tokens including the special token\n",
    "print(\"\\nToken visualization:\")\n",
    "token_strs = []\n",
    "for token_id in encoded_special:\n",
    "    if token_id in special_tokenizer.special_tokens_inv:\n",
    "        # This is a special token\n",
    "        token_strs.append(special_tokenizer.special_tokens_inv[token_id])\n",
    "    else:\n",
    "        # Regular token\n",
    "        token_bytes = special_tokenizer.vocab[token_id]\n",
    "        try:\n",
    "            token_str = token_bytes.decode('utf-8')\n",
    "            token_strs.append(token_str)\n",
    "        except UnicodeDecodeError:\n",
    "            token_strs.append(f\"[hex: {token_bytes.hex()}]\")\n",
    "\n",
    "# Print with token boundaries\n",
    "result = \"\"\n",
    "for token in token_strs:\n",
    "    if token in special_tokens:\n",
    "        # Highlight special tokens\n",
    "        result += f\"[*{token}*]\"\n",
    "    else:\n",
    "        result += f\"[{token}]\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Against Reference Tokenizers\n",
    "\n",
    "Let's compare our tokenizer implementation with other popular tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Test different regex patterns for tokenization\n# Create a training text sample\nsample_text = \"\"\"\nHello world! This is a test of the regex-based tokenization.\nIt handles contractions like don't, can't, and it's properly.\nNumbers like 123 and 45678 are split differently.\nEmojis like 😊 and symbols like @ # $ % are also handled.\n\"\"\"\n\n# Create tokenizers with different patterns\nbasic_pattern = r'(\\s+|[a-zA-Z]+|[0-9]+|\\S)'\ntokenizer_basic = RegexTokenizer(pattern=basic_pattern)\ntokenizer_gpt2 = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\ntokenizer_gpt4 = RegexTokenizer(pattern=GPT4_SPLIT_PATTERN)\n\n# Train all tokenizers on the same text with the same vocabulary size\nvocab_size = 400\nfor tokenizer, name in zip([tokenizer_basic, tokenizer_gpt2, tokenizer_gpt4], \n                          [\"Basic Pattern\", \"GPT-2 Pattern\", \"GPT-4 Pattern\"]):\n    print(f\"\\nTraining with {name}...\")\n    tokenizer.train(sample_text, vocab_size, verbose=True)\n\n# Test tokenization with different patterns\ntest_text = \"Hello world! Let's test contractions like can't and numbers like 123 and 4567.\"\n\nprint(\"\\n\\nTokenization Comparison:\")\nfor tokenizer, name in zip([tokenizer_basic, tokenizer_gpt2, tokenizer_gpt4], \n                          [\"Basic Pattern\", \"GPT-2 Pattern\", \"GPT-4 Pattern\"]):\n    tokens = tokenizer.encode(test_text)\n    print(f\"\\n{name} - {len(tokens)} tokens:\")\n    \n    # Visualize token boundaries\n    parts = []\n    for token_id in tokens:\n        token_bytes = tokenizer.vocab[token_id]\n        try:\n            token_str = token_bytes.decode('utf-8', errors='replace')\n            parts.append(f\"[{token_str}]\")\n        except UnicodeDecodeError:\n            parts.append(f\"[{token_bytes.hex()}]\")\n    \n    print(\"\".join(parts))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class SpecialTokensTokenizer(RegexTokenizer):\n    \"\"\"Enhanced tokenizer with support for special tokens and regex pre-tokenization.\"\"\"\n    \n    def __init__(self, pattern=None, special_tokens=None):\n        \"\"\"\n        Initialize the tokenizer with optional regex pattern and special tokens.\n        \n        Args:\n            pattern: A regex pattern to split text before tokenization.\n                    If None, defaults to GPT4_SPLIT_PATTERN.\n            special_tokens: Dictionary mapping special token strings to their IDs.\n                           Example: {'<|endoftext|>': 100257}\n        \"\"\"\n        super().__init__(pattern=pattern)\n        \n        # Initialize special tokens\n        self.special_tokens = {}  # str -> id\n        self.inverse_special_tokens = {}  # id -> str\n        \n        # Register special tokens if provided\n        if special_tokens:\n            self.register_special_tokens(special_tokens)\n    \n    def register_special_tokens(self, special_tokens):\n        \"\"\"\n        Register special tokens with the tokenizer.\n        \n        Args:\n            special_tokens: Dictionary mapping special token strings to their IDs.\n                           Example: {'<|endoftext|>': 100257}\n        \"\"\"\n        # Add special tokens to our dictionaries\n        for token, idx in special_tokens.items():\n            # Make sure ID doesn't conflict with existing vocab\n            if idx in self.vocab and idx not in self.inverse_special_tokens:\n                raise ValueError(f\"ID {idx} already exists in vocabulary\")\n            \n            # Add to special tokens dictionaries\n            self.special_tokens[token] = idx\n            self.inverse_special_tokens[idx] = token\n            \n            # Add to vocabulary for decoding\n            self.vocab[idx] = token.encode('utf-8')\n    \n    def encode(self, text, allowed_special=\"none_raise\"):\n        \"\"\"\n        Encode text with special token handling.\n        \n        Args:\n            text: The text to encode\n            allowed_special: How to handle special tokens.\n                            - \"all\": Process all special tokens\n                            - \"none\": Ignore special tokens (treat as normal text)\n                            - \"none_raise\": Raise error if special tokens present\n                            - set(...): Process only specified special tokens\n        \n        Returns:\n            List of token IDs\n        \"\"\"\n        # Decode the user desire w.r.t. handling of special tokens\n        special = None\n        if allowed_special == \"all\":\n            special = self.special_tokens\n        elif allowed_special == \"none\":\n            special = {}\n        elif allowed_special == \"none_raise\":\n            special = {}\n            for token in self.special_tokens:\n                if token in text:\n                    raise ValueError(f\"Special token {token} found in text but not allowed.\")\n        elif isinstance(allowed_special, set):\n            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n        else:\n            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n        \n        # If no special tokens, use the base encoding\n        if not special:\n            return super().encode(text)\n        \n        # Handle special tokens by splitting the text\n        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n        parts = re.split(special_pattern, text)\n        \n        # Process each part\n        ids = []\n        for part in parts:\n            if part in special:\n                # This is a special token, add its ID\n                ids.append(special[part])\n            elif part:  # Skip empty strings from split\n                # This is regular text, encode it normally\n                ids.extend(super().encode(part))\n        \n        return ids\n    \n    def decode(self, ids):\n        \"\"\"\n        Decode token IDs back to text.\n        \n        Args:\n            ids: List of token IDs\n            \n        Returns:\n            Decoded text\n        \"\"\"\n        parts = []\n        for idx in ids:\n            if idx in self.inverse_special_tokens:\n                # This is a special token, decode directly\n                parts.append(self.inverse_special_tokens[idx])\n            elif idx in self.vocab:\n                # This is a regular token, decode from bytes\n                parts.append(self.vocab[idx])\n            else:\n                raise ValueError(f\"Invalid token ID: {idx}\")\n        \n        # Join all parts and decode bytes\n        text_bytes = b\"\".join(p.encode('utf-8') if isinstance(p, str) else p for p in parts)\n        return text_bytes.decode('utf-8', errors='replace')"
  },
  {
   "cell_type": "code",
   "source": "# Test improved special token handling\n# Define common GPT special tokens\ngpt_special_tokens = {\n    '<|endoftext|>': 100257,\n    '<|fim_prefix|>': 100258,\n    '<|fim_middle|>': 100259,\n    '<|fim_suffix|>': 100260,\n    '<|endofprompt|>': 100261\n}\n\n# Create a special tokens tokenizer\nspecial_tokenizer = SpecialTokensTokenizer(pattern=GPT4_SPLIT_PATTERN, special_tokens=gpt_special_tokens)\n\n# Train it on some text\nspecial_tokenizer.train(sample_text, vocab_size=400, verbose=False)\n\n# Test encoding with special tokens\ntest_strings = [\n    \"Hello world\",\n    \"<|endoftext|>Hello world\",\n    \"Hello<|endoftext|>world\",\n    \"Hello world<|endoftext|>\",\n    \"Hello <|fim_prefix|>world<|fim_middle|>test<|fim_suffix|>\"\n]\n\nprint(\"Special Token Handling Tests:\\n\")\n\n# Test with different allowed_special settings\nfor allowed in [\"all\", \"none\", set(['<|endoftext|>'])]:\n    print(f\"\\nallowed_special = {allowed}\")\n    print(\"-\" * 40)\n    \n    for text in test_strings:\n        try:\n            tokens = special_tokenizer.encode(text, allowed_special=allowed)\n            \n            # Show which tokens are special\n            token_strs = []\n            for token_id in tokens:\n                if token_id in special_tokenizer.inverse_special_tokens:\n                    # This is a special token\n                    special_str = special_tokenizer.inverse_special_tokens[token_id]\n                    token_strs.append(f\"[*{special_str}*]\")\n                else:\n                    # Regular token\n                    token_bytes = special_tokenizer.vocab[token_id]\n                    try:\n                        token_str = token_bytes.decode('utf-8')\n                        token_strs.append(f\"[{token_str}]\")\n                    except UnicodeDecodeError:\n                        token_strs.append(f\"[hex: {token_bytes.hex()}]\")\n            \n            # Print the result\n            print(f\"Text: {text}\")\n            print(f\"Tokens: {tokens}\")\n            print(f\"Visualization: {''.join(token_strs)}\")\n            \n            # Test roundtrip\n            decoded = special_tokenizer.decode(tokens)\n            print(f\"Decoded: {decoded}\")\n            print(f\"Roundtrip success: {text == decoded}\\n\")\n            \n        except ValueError as e:\n            print(f\"Text: {text}\")\n            print(f\"Error: {str(e)}\\n\")\n\n# Test error handling with none_raise\nprint(\"\\nallowed_special = none_raise (should raise errors)\")\nprint(\"-\" * 40)\nfor text in test_strings:\n    try:\n        tokens = special_tokenizer.encode(text, allowed_special=\"none_raise\")\n        print(f\"Text: {text}\")\n        print(f\"Tokens: {tokens}\\n\")\n    except ValueError as e:\n        print(f\"Text: {text}\")\n        print(f\"Error: {str(e)}\\n\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Add Karpathy-compatible save and load functions to base Tokenizer class\ndef tokenizer_save(self, file_prefix):\n    \"\"\"\n    Save the tokenizer in Karpathy's format.\n    Creates two files:\n    - file_prefix.model: Contains the pattern, special tokens, and merges (used for loading)\n    - file_prefix.vocab: Human-readable vocabulary (for inspection only)\n    \n    Args:\n        file_prefix: Path prefix for the saved files\n    \"\"\"\n    # Write the model file - used for loading later\n    model_file = file_prefix + \".model\"\n    with open(model_file, 'w') as f:\n        # Write version and pattern\n        f.write(\"minbpe v1\\n\")\n        f.write(f\"{getattr(self, 'pattern', '')}\\n\")\n        \n        # Write special tokens\n        special_tokens = getattr(self, 'special_tokens', {})\n        f.write(f\"{len(special_tokens)}\\n\")\n        for special, idx in special_tokens.items():\n            f.write(f\"{special} {idx}\\n\")\n            \n        # Write the merges\n        for (idx1, idx2), idx in self.merges.items():\n            f.write(f\"{idx1} {idx2}\\n\")\n    \n    # Write the vocab file - for human inspection\n    vocab_file = file_prefix + \".vocab\"\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        # Build an inverted merges dictionary for visualization\n        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n        \n        # Write each token with its source if available\n        for idx, token in self.vocab.items():\n            # Try to decode the token for display\n            try:\n                token_str = token.decode('utf-8', errors='replace')\n                token_str = token_str.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n                if len(token_str.strip()) == 0:\n                    # For whitespace, show hex\n                    token_str = f\"hex: {token.hex()}\"\n            except:\n                token_str = f\"hex: {token.hex()}\"\n                \n            # If this token has children, show the merge\n            if idx in inverted_merges:\n                idx0, idx1 = inverted_merges[idx]\n                \n                # Get string representations of the children\n                try:\n                    s0 = self.vocab[idx0].decode('utf-8', errors='replace')\n                    s0 = s0.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n                    if len(s0.strip()) == 0:\n                        s0 = f\"hex: {self.vocab[idx0].hex()}\"\n                except:\n                    s0 = f\"hex: {self.vocab[idx0].hex()}\"\n                    \n                try:\n                    s1 = self.vocab[idx1].decode('utf-8', errors='replace')\n                    s1 = s1.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n                    if len(s1.strip()) == 0:\n                        s1 = f\"hex: {self.vocab[idx1].hex()}\"\n                except:\n                    s1 = f\"hex: {self.vocab[idx1].hex()}\"\n                \n                # Write the merge information\n                f.write(f\"[{s0}][{s1}] -> [{token_str}] {idx}\\n\")\n            else:\n                # This is a leaf token (raw byte or special token)\n                f.write(f\"[{token_str}] {idx}\\n\")\n                \n    print(f\"Saved tokenizer to {file_prefix}.model and {file_prefix}.vocab\")\n\ndef tokenizer_load(self, model_file):\n    \"\"\"\n    Load a tokenizer from a .model file in Karpathy's format.\n    \n    Args:\n        model_file: Path to the .model file\n    \"\"\"\n    assert model_file.endswith(\".model\"), \"File must have .model extension\"\n    \n    # Reset current state\n    self.merges = {}\n    special_tokens = {}\n    \n    # Read the model file\n    with open(model_file, 'r', encoding=\"utf-8\") as f:\n        # Read version\n        version = f.readline().strip()\n        assert version == \"minbpe v1\", f\"Unknown model version: {version}\"\n        \n        # Read pattern if available\n        pattern = f.readline().strip()\n        if hasattr(self, 'pattern'):\n            self.pattern = pattern\n            self.compiled_pattern = re.compile(pattern)\n        \n        # Read special tokens\n        num_special = int(f.readline().strip())\n        for _ in range(num_special):\n            line = f.readline().strip()\n            special, special_idx = line.split(' ', 1)\n            special_tokens[special] = int(special_idx)\n        \n        # Read merges\n        next_idx = 256\n        for line in f:\n            idx1, idx2 = map(int, line.split())\n            self.merges[(idx1, idx2)] = next_idx\n            next_idx += 1\n    \n    # Rebuild vocab\n    self.vocab = {idx: bytes([idx]) for idx in range(256)}\n    for (p0, p1), idx in self.merges.items():\n        self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n    \n    # Add special tokens if supported\n    if hasattr(self, 'register_special_tokens'):\n        self.register_special_tokens(special_tokens)\n    elif hasattr(self, 'special_tokens'):\n        self.special_tokens = special_tokens\n        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n        # Add special tokens to vocabulary\n        for token, idx in special_tokens.items():\n            self.vocab[idx] = token.encode('utf-8')\n    \n    print(f\"Loaded tokenizer from {model_file} with {len(self.merges)} merges\")\n\n# Add the save/load methods to the Tokenizer class\nTokenizer.save = tokenizer_save\nTokenizer.load = tokenizer_load\n\n# Test the save and load functionality\ndef test_save_load():\n    # Create and train a tokenizer\n    tokenizer = RegexTokenizer(pattern=GPT4_SPLIT_PATTERN)\n    tokenizer.train(sample_text, vocab_size=400, verbose=False)\n    \n    # Save the tokenizer\n    tokenizer.save(\"test_tokenizer\")\n    \n    # Load a new tokenizer and compare\n    new_tokenizer = RegexTokenizer()\n    new_tokenizer.load(\"test_tokenizer.model\")\n    \n    # Test encoding/decoding with both tokenizers\n    test_text = \"Hello world! This is a tokenizer test.\"\n    original_tokens = tokenizer.encode(test_text)\n    loaded_tokens = new_tokenizer.encode(test_text)\n    \n    print(f\"Original tokens: {original_tokens}\")\n    print(f\"Loaded tokens: {loaded_tokens}\")\n    print(f\"Tokens match: {original_tokens == loaded_tokens}\")\n    \n    # Test special tokens tokenizer\n    special_tokenizer = SpecialTokensTokenizer(\n        pattern=GPT4_SPLIT_PATTERN, \n        special_tokens=gpt_special_tokens\n    )\n    special_tokenizer.train(sample_text, vocab_size=400, verbose=False)\n    \n    # Save and load\n    special_tokenizer.save(\"test_special_tokenizer\")\n    \n    new_special_tokenizer = SpecialTokensTokenizer()\n    new_special_tokenizer.load(\"test_special_tokenizer.model\")\n    \n    # Test with special tokens\n    special_text = \"Hello<|endoftext|>world\"\n    original_special = special_tokenizer.encode(special_text, allowed_special=\"all\")\n    loaded_special = new_special_tokenizer.encode(special_text, allowed_special=\"all\")\n    \n    print(f\"\\nSpecial tokens test:\")\n    print(f\"Original tokens: {original_special}\")\n    print(f\"Loaded tokens: {loaded_special}\")\n    print(f\"Tokens match: {original_special == loaded_special}\")\n    \n    # Clean up test files\n    import os\n    for file in [\"test_tokenizer.model\", \"test_tokenizer.vocab\", \n                \"test_special_tokenizer.model\", \"test_special_tokenizer.vocab\"]:\n        if os.path.exists(file):\n            os.remove(file)\n\n# Run the test\ntest_save_load()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer       | Text       | Tokens   | Time (ms) \n",
      "--------------------------------------------------\n",
      "Basic BPE       | English    | 35       | 0.05      \n",
      "Basic BPE       | Code       | 50       | 0.04      \n",
      "Basic BPE       | Mixed      | 34       | 0.04      \n",
      "Basic BPE       | Repeated   | 23       | 0.01      \n",
      "Regex BPE       | English    | 44       | 0.25      \n",
      "Regex BPE       | Code       | 60       | 0.07      \n",
      "Regex BPE       | Mixed      | 43       | 0.04      \n",
      "Regex BPE       | Repeated   | 35       | 0.02      \n",
      "Special BPE     | English    | 35       | 0.09      \n",
      "Special BPE     | Code       | 50       | 0.08      \n",
      "Special BPE     | Mixed      | 34       | 0.06      \n",
      "Special BPE     | Repeated   | 23       | 0.03      \n"
     ]
    }
   ],
   "source": [
    "# This cell would normally import and compare with reference tokenizers\n",
    "# We provide pseudocode here as an example of what you would do\n",
    "\n",
    "\"\"\"\n",
    "# You would usually import tiktoken or transformers:\n",
    "# import tiktoken\n",
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# Define benchmark texts and functions\n",
    "benchmark_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Mixed\": \"Training at 3.5x speed 😊 быстрое обучение\",\n",
    "    \"Repeated\": \"token token token token token token\"\n",
    "}\n",
    "\n",
    "def benchmark_tokenizer(name, tokenizer_func, texts):\n",
    "    results = []\n",
    "    for text_name, text in texts.items():\n",
    "        # Measure encoding time\n",
    "        start_time = time.time()\n",
    "        tokens = tokenizer_func(text)\n",
    "        encode_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            \"tokenizer\": name,\n",
    "            \"text\": text_name,\n",
    "            \"tokens\": len(tokens),\n",
    "            \"encode_time_ms\": encode_time * 1000\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Define encoding functions for different tokenizers\n",
    "def encode_with_our_tokenizer(text):\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "# def encode_with_tiktoken(text):\n",
    "#     enc = tiktoken.get_encoding(\"gpt2\")\n",
    "#     return enc.encode(text)\n",
    "    \n",
    "# def encode_with_hf_tokenizer(text):\n",
    "#     hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#     return hf_tokenizer.encode(text)\n",
    "\n",
    "# Run benchmarks\n",
    "our_results = benchmark_tokenizer(\"Our BPE\", encode_with_our_tokenizer, benchmark_texts)\n",
    "# tiktoken_results = benchmark_tokenizer(\"tiktoken\", encode_with_tiktoken, benchmark_texts)\n",
    "# hf_results = benchmark_tokenizer(\"HuggingFace\", encode_with_hf_tokenizer, benchmark_texts)\n",
    "\n",
    "# all_results = our_results + tiktoken_results + hf_results\n",
    "all_results = our_results\n",
    "\n",
    "# Display results in a table\n",
    "print(f\"{'Tokenizer':<15} | {'Text':<10} | {'Tokens':<8} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in all_results:\n",
    "    print(f\"{r['tokenizer']:<15} | {r['text']:<10} | {r['tokens']:<8} | {r['encode_time_ms']:<10.2f}\")\n",
    "\n",
    "# Here you would normally create visualization comparing the tokenizers\n",
    "\"\"\"\n",
    "\n",
    "# For now, we'll just run a simple benchmark on our own tokenizer\n",
    "benchmark_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Mixed\": \"Training at 3.5x speed 😊 快速训练！\",\n",
    "    \"Repeated\": \"token token token token token token\"\n",
    "}\n",
    "\n",
    "def benchmark_our_tokenizers(texts):\n",
    "    results = []\n",
    "    tokenizers = {\n",
    "        \"Basic BPE\": tokenizer,\n",
    "        \"Regex BPE\": regex_tokenizer,\n",
    "        \"Special BPE\": special_tokenizer\n",
    "    }\n",
    "    \n",
    "    for name, tkn in tokenizers.items():\n",
    "        for text_name, text in texts.items():\n",
    "            # Measure encoding time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For special tokenizer, specify allowed_special\n",
    "            if name == \"Special BPE\":\n",
    "                tokens = tkn.encode(text, allowed_special=\"all\")\n",
    "            else:\n",
    "                tokens = tkn.encode(text)\n",
    "                \n",
    "            encode_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"tokenizer\": name,\n",
    "                \"text\": text_name,\n",
    "                \"tokens\": len(tokens),\n",
    "                \"encode_time_ms\": encode_time * 1000\n",
    "            })\n",
    "    return results\n",
    "\n",
    "results = benchmark_our_tokenizers(benchmark_texts)\n",
    "\n",
    "# Display results in a table\n",
    "print(f\"{'Tokenizer':<15} | {'Text':<10} | {'Tokens':<8} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"{r['tokenizer']:<15} | {r['text']:<10} | {r['tokens']:<8} | {r['encode_time_ms']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer       | Text       | Tokens   | Time (ms) \n",
      "--------------------------------------------------\n",
      "Basic BPE       | English    | 35       | 0.05      \n",
      "Basic BPE       | Code       | 50       | 0.04      \n",
      "Basic BPE       | Mixed      | 34       | 0.04      \n",
      "Basic BPE       | Repeated   | 23       | 0.01      \n",
      "Regex BPE       | English    | 44       | 0.07      \n",
      "Regex BPE       | Code       | 60       | 0.15      \n",
      "Regex BPE       | Mixed      | 43       | 0.05      \n",
      "Regex BPE       | Repeated   | 35       | 0.02      \n",
      "Special BPE     | English    | 35       | 0.08      \n",
      "Special BPE     | Code       | 50       | 0.09      \n",
      "Special BPE     | Mixed      | 34       | 0.06      \n",
      "Special BPE     | Repeated   | 23       | 0.03      \n"
     ]
    }
   ],
   "source": [
    "# Train the regex tokenizer first\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.train(training_text, vocab_size=500, verbose=False)\n",
    "\n",
    "# Now we can do benchmarking\n",
    "benchmark_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Mixed\": \"Training at 3.5x speed 😊 快速训练！\",\n",
    "    \"Repeated\": \"token token token token token token\"\n",
    "}\n",
    "\n",
    "def benchmark_our_tokenizers(texts):\n",
    "    results = []\n",
    "    tokenizers = {\n",
    "        \"Basic BPE\": tokenizer,\n",
    "        \"Regex BPE\": regex_tokenizer,\n",
    "        \"Special BPE\": special_tokenizer\n",
    "    }\n",
    "    \n",
    "    for name, tkn in tokenizers.items():\n",
    "        for text_name, text in texts.items():\n",
    "            # Measure encoding time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For special tokenizer, specify allowed_special\n",
    "            if name == \"Special BPE\":\n",
    "                tokens = tkn.encode(text, allowed_special=\"all\")\n",
    "            else:\n",
    "                tokens = tkn.encode(text)\n",
    "                \n",
    "            encode_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"tokenizer\": name,\n",
    "                \"text\": text_name,\n",
    "                \"tokens\": len(tokens),\n",
    "                \"encode_time_ms\": encode_time * 1000\n",
    "            })\n",
    "    return results\n",
    "\n",
    "results = benchmark_our_tokenizers(benchmark_texts)\n",
    "\n",
    "# Display results in a table\n",
    "print(f\"{'Tokenizer':<15} | {'Text':<10} | {'Tokens':<8} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"{r['tokenizer']:<15} | {r['text']:<10} | {r['tokens']:<8} | {r['encode_time_ms']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Token Visualization\n",
    "\n",
    "Let's improve our token visualization to better illustrate token boundaries and include additional token information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Regex Tokenizer\n",
    "\n",
    "Let's compare the basic tokenizer with our regex-based tokenizer to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tokenizer: 51 tokens\n",
      "Regex tokenizer: 76 tokens\n",
      "\n",
      "Basic tokenization:\n",
      "Tokenized into 51 tokens:\n",
      "[I][t]['][s␣][n][o][t␣][j][u][s][t␣][to][k][en][iz][a][ti][o][n][,␣][it]['][s␣][BPE][␣][to][k][en][iz][a][ti][o][n␣][with][␣re][g][e][x][␣][p][re][-][p][r][o][c][e][s][s][ing][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 257 = 's␣'\n",
      "Token 5: ID 110 = 'n'\n",
      "Token 6: ID 111 = 'o'\n",
      "Token 7: ID 260 = 't␣'\n",
      "Token 8: ID 106 = 'j'\n",
      "Token 9: ID 117 = 'u'\n",
      "Token 10: ID 115 = 's'\n",
      "Token 11: ID 260 = 't␣'\n",
      "Token 12: ID 305 = 'to'\n",
      "Token 13: ID 107 = 'k'\n",
      "Token 14: ID 269 = 'en'\n",
      "Token 15: ID 306 = 'iz'\n",
      "Token 16: ID 97 = 'a'\n",
      "Token 17: ID 266 = 'ti'\n",
      "Token 18: ID 111 = 'o'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 303 = ',␣'\n",
      "Token 21: ID 346 = 'it'\n",
      "Token 22: ID 39 = '''\n",
      "Token 23: ID 257 = 's␣'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '␣'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 293 = 'n␣'\n",
      "Token 34: ID 324 = 'with'\n",
      "Token 35: ID 312 = '␣re'\n",
      "Token 36: ID 103 = 'g'\n",
      "Token 37: ID 101 = 'e'\n",
      "Token 38: ID 120 = 'x'\n",
      "Token 39: ID 32 = '␣'\n",
      "Token 40: ID 112 = 'p'\n",
      "Token 41: ID 265 = 're'\n",
      "Token 42: ID 45 = '-'\n",
      "Token 43: ID 112 = 'p'\n",
      "Token 44: ID 114 = 'r'\n",
      "Token 45: ID 111 = 'o'\n",
      "Token 46: ID 99 = 'c'\n",
      "Token 47: ID 101 = 'e'\n",
      "Token 48: ID 115 = 's'\n",
      "Token 49: ID 115 = 's'\n",
      "Token 50: ID 264 = 'ing'\n",
      "Token 51: ID 33 = '!'\n",
      "\n",
      "Regex tokenization:\n",
      "Tokenized into 76 tokens:\n",
      "[I][t]['][s][␣][n][o][t][␣][j][u][s][t][␣][t][o][k][e][n][i][z][a][t][i][o][n][,][␣][i][t]['][s][␣][B][P][E][␣][t][o][k][e][n][i][z][a][t][i][o][n][␣][w][i][t][h][␣][r][e][g][e][x][␣][p][r][e][-][p][r][o][c][e][s][s][i][n][g][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 115 = 's'\n",
      "Token 5: ID 32 = '␣'\n",
      "Token 6: ID 110 = 'n'\n",
      "Token 7: ID 111 = 'o'\n",
      "Token 8: ID 116 = 't'\n",
      "Token 9: ID 32 = '␣'\n",
      "Token 10: ID 106 = 'j'\n",
      "Token 11: ID 117 = 'u'\n",
      "Token 12: ID 115 = 's'\n",
      "Token 13: ID 116 = 't'\n",
      "Token 14: ID 32 = '␣'\n",
      "Token 15: ID 116 = 't'\n",
      "Token 16: ID 111 = 'o'\n",
      "Token 17: ID 107 = 'k'\n",
      "Token 18: ID 101 = 'e'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 105 = 'i'\n",
      "Token 21: ID 122 = 'z'\n",
      "Token 22: ID 97 = 'a'\n",
      "Token 23: ID 116 = 't'\n",
      "Token 24: ID 105 = 'i'\n",
      "Token 25: ID 111 = 'o'\n",
      "Token 26: ID 110 = 'n'\n",
      "Token 27: ID 44 = ','\n",
      "Token 28: ID 32 = '␣'\n",
      "Token 29: ID 105 = 'i'\n",
      "Token 30: ID 116 = 't'\n",
      "Token 31: ID 39 = '''\n",
      "Token 32: ID 115 = 's'\n",
      "Token 33: ID 32 = '␣'\n",
      "Token 34: ID 66 = 'B'\n",
      "Token 35: ID 80 = 'P'\n",
      "Token 36: ID 69 = 'E'\n",
      "Token 37: ID 32 = '␣'\n",
      "Token 38: ID 116 = 't'\n",
      "Token 39: ID 111 = 'o'\n",
      "Token 40: ID 107 = 'k'\n",
      "Token 41: ID 101 = 'e'\n",
      "Token 42: ID 110 = 'n'\n",
      "Token 43: ID 105 = 'i'\n",
      "Token 44: ID 122 = 'z'\n",
      "Token 45: ID 97 = 'a'\n",
      "Token 46: ID 116 = 't'\n",
      "Token 47: ID 105 = 'i'\n",
      "Token 48: ID 111 = 'o'\n",
      "Token 49: ID 110 = 'n'\n",
      "Token 50: ID 32 = '␣'\n",
      "Token 51: ID 119 = 'w'\n",
      "Token 52: ID 105 = 'i'\n",
      "Token 53: ID 116 = 't'\n",
      "Token 54: ID 104 = 'h'\n",
      "Token 55: ID 32 = '␣'\n",
      "Token 56: ID 114 = 'r'\n",
      "Token 57: ID 101 = 'e'\n",
      "Token 58: ID 103 = 'g'\n",
      "Token 59: ID 101 = 'e'\n",
      "Token 60: ID 120 = 'x'\n",
      "Token 61: ID 32 = '␣'\n",
      "Token 62: ID 112 = 'p'\n",
      "Token 63: ID 114 = 'r'\n",
      "Token 64: ID 101 = 'e'\n",
      "Token 65: ID 45 = '-'\n",
      "Token 66: ID 112 = 'p'\n",
      "Token 67: ID 114 = 'r'\n",
      "Token 68: ID 111 = 'o'\n",
      "Token 69: ID 99 = 'c'\n",
      "Token 70: ID 101 = 'e'\n",
      "Token 71: ID 115 = 's'\n",
      "Token 72: ID 115 = 's'\n",
      "Token 73: ID 105 = 'i'\n",
      "Token 74: ID 110 = 'n'\n",
      "Token 75: ID 103 = 'g'\n",
      "Token 76: ID 33 = '!'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " '␣',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ',',\n",
       " '␣',\n",
       " 'i',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'B',\n",
       " 'P',\n",
       " 'E',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " '␣',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '␣',\n",
       " 'r',\n",
       " 'e',\n",
       " 'g',\n",
       " 'e',\n",
       " 'x',\n",
       " '␣',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " '-',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with the basic tokenizer\n",
    "compare_text = \"It's not just tokenization, it's BPE tokenization with regex pre-processing!\"\n",
    "\n",
    "basic_tokens = tokenizer.encode(compare_text)\n",
    "regex_tokens = regex_tokenizer.encode(compare_text)\n",
    "\n",
    "print(f\"Basic tokenizer: {len(basic_tokens)} tokens\")\n",
    "print(f\"Regex tokenizer: {len(regex_tokens)} tokens\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(\"\\nBasic tokenization:\")\n",
    "visualize_tokenization(tokenizer, compare_text)\n",
    "\n",
    "print(\"\\nRegex tokenization:\")\n",
    "visualize_tokenization(regex_tokenizer, compare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer to vocab size 500\n",
      "Text size: 656 chars, 662 bytes\n",
      "Merge #0: pair (101, 32) -> 256, corpus now 647 tokens\n",
      "Merge #100: pair (355, 110) -> 356, corpus now 311 tokens\n",
      "Merge #200: pair (455, 277) -> 456, corpus now 211 tokens\n",
      "Basic tokenizer: 51 tokens\n",
      "Regex tokenizer: 76 tokens\n",
      "\n",
      "Basic tokenization:\n",
      "Tokenized into 51 tokens:\n",
      "[I][t]['][s␣][n][o][t␣][j][u][s][t␣][to][k][en][iz][a][ti][o][n][,␣][it]['][s␣][BPE][␣][to][k][en][iz][a][ti][o][n␣][with][␣re][g][e][x][␣][p][re][-][p][r][o][c][e][s][s][ing][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 257 = 's␣'\n",
      "Token 5: ID 110 = 'n'\n",
      "Token 6: ID 111 = 'o'\n",
      "Token 7: ID 260 = 't␣'\n",
      "Token 8: ID 106 = 'j'\n",
      "Token 9: ID 117 = 'u'\n",
      "Token 10: ID 115 = 's'\n",
      "Token 11: ID 260 = 't␣'\n",
      "Token 12: ID 305 = 'to'\n",
      "Token 13: ID 107 = 'k'\n",
      "Token 14: ID 269 = 'en'\n",
      "Token 15: ID 306 = 'iz'\n",
      "Token 16: ID 97 = 'a'\n",
      "Token 17: ID 266 = 'ti'\n",
      "Token 18: ID 111 = 'o'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 303 = ',␣'\n",
      "Token 21: ID 346 = 'it'\n",
      "Token 22: ID 39 = '''\n",
      "Token 23: ID 257 = 's␣'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '␣'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 293 = 'n␣'\n",
      "Token 34: ID 324 = 'with'\n",
      "Token 35: ID 312 = '␣re'\n",
      "Token 36: ID 103 = 'g'\n",
      "Token 37: ID 101 = 'e'\n",
      "Token 38: ID 120 = 'x'\n",
      "Token 39: ID 32 = '␣'\n",
      "Token 40: ID 112 = 'p'\n",
      "Token 41: ID 265 = 're'\n",
      "Token 42: ID 45 = '-'\n",
      "Token 43: ID 112 = 'p'\n",
      "Token 44: ID 114 = 'r'\n",
      "Token 45: ID 111 = 'o'\n",
      "Token 46: ID 99 = 'c'\n",
      "Token 47: ID 101 = 'e'\n",
      "Token 48: ID 115 = 's'\n",
      "Token 49: ID 115 = 's'\n",
      "Token 50: ID 264 = 'ing'\n",
      "Token 51: ID 33 = '!'\n",
      "\n",
      "Regex tokenization:\n",
      "Tokenized into 76 tokens:\n",
      "[I][t]['][s][␣][n][o][t][␣][j][u][s][t][␣][t][o][k][e][n][i][z][a][t][i][o][n][,][␣][i][t]['][s][␣][B][P][E][␣][t][o][k][e][n][i][z][a][t][i][o][n][␣][w][i][t][h][␣][r][e][g][e][x][␣][p][r][e][-][p][r][o][c][e][s][s][i][n][g][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 115 = 's'\n",
      "Token 5: ID 32 = '␣'\n",
      "Token 6: ID 110 = 'n'\n",
      "Token 7: ID 111 = 'o'\n",
      "Token 8: ID 116 = 't'\n",
      "Token 9: ID 32 = '␣'\n",
      "Token 10: ID 106 = 'j'\n",
      "Token 11: ID 117 = 'u'\n",
      "Token 12: ID 115 = 's'\n",
      "Token 13: ID 116 = 't'\n",
      "Token 14: ID 32 = '␣'\n",
      "Token 15: ID 116 = 't'\n",
      "Token 16: ID 111 = 'o'\n",
      "Token 17: ID 107 = 'k'\n",
      "Token 18: ID 101 = 'e'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 105 = 'i'\n",
      "Token 21: ID 122 = 'z'\n",
      "Token 22: ID 97 = 'a'\n",
      "Token 23: ID 116 = 't'\n",
      "Token 24: ID 105 = 'i'\n",
      "Token 25: ID 111 = 'o'\n",
      "Token 26: ID 110 = 'n'\n",
      "Token 27: ID 44 = ','\n",
      "Token 28: ID 32 = '␣'\n",
      "Token 29: ID 105 = 'i'\n",
      "Token 30: ID 116 = 't'\n",
      "Token 31: ID 39 = '''\n",
      "Token 32: ID 115 = 's'\n",
      "Token 33: ID 32 = '␣'\n",
      "Token 34: ID 66 = 'B'\n",
      "Token 35: ID 80 = 'P'\n",
      "Token 36: ID 69 = 'E'\n",
      "Token 37: ID 32 = '␣'\n",
      "Token 38: ID 116 = 't'\n",
      "Token 39: ID 111 = 'o'\n",
      "Token 40: ID 107 = 'k'\n",
      "Token 41: ID 101 = 'e'\n",
      "Token 42: ID 110 = 'n'\n",
      "Token 43: ID 105 = 'i'\n",
      "Token 44: ID 122 = 'z'\n",
      "Token 45: ID 97 = 'a'\n",
      "Token 46: ID 116 = 't'\n",
      "Token 47: ID 105 = 'i'\n",
      "Token 48: ID 111 = 'o'\n",
      "Token 49: ID 110 = 'n'\n",
      "Token 50: ID 32 = '␣'\n",
      "Token 51: ID 119 = 'w'\n",
      "Token 52: ID 105 = 'i'\n",
      "Token 53: ID 116 = 't'\n",
      "Token 54: ID 104 = 'h'\n",
      "Token 55: ID 32 = '␣'\n",
      "Token 56: ID 114 = 'r'\n",
      "Token 57: ID 101 = 'e'\n",
      "Token 58: ID 103 = 'g'\n",
      "Token 59: ID 101 = 'e'\n",
      "Token 60: ID 120 = 'x'\n",
      "Token 61: ID 32 = '␣'\n",
      "Token 62: ID 112 = 'p'\n",
      "Token 63: ID 114 = 'r'\n",
      "Token 64: ID 101 = 'e'\n",
      "Token 65: ID 45 = '-'\n",
      "Token 66: ID 112 = 'p'\n",
      "Token 67: ID 114 = 'r'\n",
      "Token 68: ID 111 = 'o'\n",
      "Token 69: ID 99 = 'c'\n",
      "Token 70: ID 101 = 'e'\n",
      "Token 71: ID 115 = 's'\n",
      "Token 72: ID 115 = 's'\n",
      "Token 73: ID 105 = 'i'\n",
      "Token 74: ID 110 = 'n'\n",
      "Token 75: ID 103 = 'g'\n",
      "Token 76: ID 33 = '!'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " '␣',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ',',\n",
       " '␣',\n",
       " 'i',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'B',\n",
       " 'P',\n",
       " 'E',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " '␣',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '␣',\n",
       " 'r',\n",
       " 'e',\n",
       " 'g',\n",
       " 'e',\n",
       " 'x',\n",
       " '␣',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " '-',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the regex tokenizer\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.train(training_text, vocab_size=500, verbose=True)\n",
    "\n",
    "# Compare with the basic tokenizer\n",
    "compare_text = \"It's not just tokenization, it's BPE tokenization with regex pre-processing!\"\n",
    "\n",
    "basic_tokens = tokenizer.encode(compare_text)\n",
    "regex_tokens = regex_tokenizer.encode(compare_text)\n",
    "\n",
    "print(f\"Basic tokenizer: {len(basic_tokens)} tokens\")\n",
    "print(f\"Regex tokenizer: {len(regex_tokens)} tokens\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(\"\\nBasic tokenization:\")\n",
    "visualize_tokenization(tokenizer, compare_text)\n",
    "\n",
    "print(\"\\nRegex tokenization:\")\n",
    "visualize_tokenization(regex_tokenizer, compare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## GPT4Tokenizer Implementation\n\nModern tokenizers like those used in OpenAI's models employ several advanced techniques to improve tokenization efficiency and quality:\n\n1. **Pattern-Based Splitting**: Uses sophisticated regex patterns to split text along linguistic boundaries\n2. **Byte Shuffling**: Reorganizes byte values to improve compression of non-ASCII characters\n3. **Special Token Handling**: Manages model-specific tokens like `<|endoftext|>` with configurable behavior\n\nThe GPT4Tokenizer implements all these features to provide compatibility with tokenizers like tiktoken's cl100k_base. This enables our educational implementation to produce results comparable to production tokenizers.\n\n### Byte Shuffling Explanation\n\nByte shuffling is a technique that permutes the byte values (particularly ASCII visible characters) to improve how the BPE algorithm learns merges for multilingual text. Here's how it works:\n\n1. Define a fixed permutation of byte values (using a seeded RNG for reproducibility)\n2. During encoding: shuffle bytes before applying BPE merges\n3. During decoding: unshuffle bytes after converting tokens back to bytes\n\nThis approach helps the tokenizer handle non-ASCII characters more efficiently by making the distribution of byte n-grams more amenable to compression through BPE merges.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class GPT4Tokenizer(SpecialTokensTokenizer):\n    \"\"\"\n    GPT4-compatible tokenizer with regex-based pre-tokenization and byte shuffling.\n    Designed to be compatible with tiktoken's cl100k_base encoding.\n    \"\"\"\n    \n    def __init__(self, pattern=None, special_tokens=None):\n        \"\"\"\n        Initialize the GPT4Tokenizer.\n        \n        Args:\n            pattern: A regex pattern to split text before tokenization.\n                    If None, defaults to GPT4_SPLIT_PATTERN.\n            special_tokens: Dictionary mapping special token strings to their IDs.\n                           Example: {'<|endoftext|>': 100257}\n        \"\"\"\n        # Default to GPT4 pattern if none provided\n        pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n        \n        # Initialize with default GPT special tokens if none provided\n        if special_tokens is None:\n            special_tokens = {\n                '<|endoftext|>': 100257,\n                '<|fim_prefix|>': 100258,\n                '<|fim_middle|>': 100259,\n                '<|fim_suffix|>': 100260,\n                '<|endofprompt|>': 100261\n            }\n        \n        super().__init__(pattern=pattern, special_tokens=special_tokens)\n        \n        # Setup byte shuffling\n        self.byte_shuffle = self._get_byte_shuffle()\n        self.byte_unshuffle = {v: k for k, v in self.byte_shuffle.items()}\n    \n    def _get_byte_shuffle(self):\n        \"\"\"\n        Create the byte shuffling mapping used by cl100k_base.\n        This shuffling is what allows the tokenizer to encode non-ASCII\n        characters more efficiently.\n        \n        Returns:\n            Dictionary mapping original byte values to shuffled values\n        \"\"\"\n        # Define the permutation pattern from cl100k_base\n        bs = list(range(ord(\"!\"), ord(\"~\") + 1))  # ASCII visible characters\n        cs = bs.copy()\n        n = len(bs)\n        \n        # Apply the permutation\n        # This is a reproducible scrambling of byte values\n        rng = random.Random(42)  # Fixed seed for reproduciblity\n        for i in range(n - 1, 0, -1):\n            j = rng.randint(0, i)\n            cs[i], cs[j] = cs[j], cs[i]\n        \n        # Create the byte shuffle mapping\n        shuffle = {b: c for b, c in zip(bs, cs)}\n        \n        # Add identity mapping for all other bytes\n        for i in range(256):\n            if i not in shuffle:\n                shuffle[i] = i\n                \n        return shuffle\n    \n    def _encode_chunk(self, text_bytes):\n        \"\"\"\n        Encode a single chunk of bytes with byte shuffling.\n        \n        Args:\n            text_bytes: Bytes to encode\n            \n        Returns:\n            List of token IDs\n        \"\"\"\n        # Apply byte shuffling\n        shuffled_bytes = bytes(self.byte_shuffle[b] for b in text_bytes)\n        \n        # Encode using the base implementation\n        return super()._encode_chunk(shuffled_bytes)\n    \n    def decode(self, ids):\n        \"\"\"\n        Decode token IDs back to text with byte unshuffling.\n        \n        Args:\n            ids: List of token IDs\n            \n        Returns:\n            Decoded text\n        \"\"\"\n        # Decode using the base implementation\n        result = super().decode(ids)\n        \n        # Apply byte unshuffling\n        result_bytes = result.encode('utf-8')\n        unshuffled_bytes = bytes(self.byte_unshuffle[b] for b in result_bytes)\n        \n        # Convert back to text\n        return unshuffled_bytes.decode('utf-8', errors='replace')\n    \n    @classmethod\n    def create_from_tiktoken(cls, encoding_name=\"cl100k_base\"):\n        \"\"\"\n        Create a GPT4Tokenizer from a tiktoken encoding.\n        This allows you to use the same vocabulary as OpenAI's models.\n        \n        Args:\n            encoding_name: The name of the tiktoken encoding to use\n            \n        Returns:\n            A GPT4Tokenizer initialized with the tiktoken vocabulary\n            \n        Note:\n            This method requires the tiktoken package to be installed.\n            If not available, it will raise an ImportError.\n        \"\"\"\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\"The tiktoken package is required for this feature. \"\n                              \"Install it with 'pip install tiktoken'.\")\n        \n        # Create the base tokenizer\n        tokenizer = cls()\n        \n        # Get the tiktoken encoding\n        enc = tiktoken.get_encoding(encoding_name)\n        \n        # This is a simplified approximation - in a full implementation,\n        # we would extract the exact merges and special tokens from tiktoken,\n        # but that's beyond the scope of this example\n        print(f\"Created GPT4Tokenizer based on {encoding_name}\")\n        print(\"Note: This is a simplified compatibility layer\")\n        \n        return tokenizer",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Testing GPT4Tokenizer\n\nLet's test our GPT4Tokenizer implementation with various text inputs to verify it works correctly, especially with special tokens and byte shuffling.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create and train a GPT4Tokenizer\ngpt4_tokenizer = GPT4Tokenizer()\n\n# Train it on some sample text\ngpt4_training_text = \"\"\"\nThis is sample text for training our GPT4Tokenizer.\nIt includes some code: def hello(): print(\"Hello, world!\")\nAs well as emojis 😊 🚀 and special characters: $%^&*()\nLet's also test some multilingual text: こんにちは, Привет, مرحبا, 你好\n\"\"\"\n\ngpt4_tokenizer.train(gpt4_training_text, vocab_size=500, verbose=True)\n\n# Test texts with different characteristics\ntest_texts = {\n    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n    \"Emojis\": \"I love coding! 😊 💻 🚀 🔥\",\n    \"Special Tokens\": \"<|endoftext|>Hello world<|fim_prefix|>\",\n    \"Multilingual\": \"Hello in Japanese is こんにちは and in Russian is Привет\",\n    \"Mixed\": \"Training at 3.5x speed 😊 快速训练！\"\n}\n\n# Test encoding and decoding\nprint(\"GPT4Tokenizer Testing:\")\nprint(\"=\" * 50)\n\nfor name, text in test_texts.items():\n    # For special tokens test, use allowed_special=\"all\"\n    allowed_special = \"all\" if name == \"Special Tokens\" else \"none\"\n    \n    # Encode the text\n    try:\n        tokens = gpt4_tokenizer.encode(text, allowed_special=allowed_special)\n        \n        # Decode back to text\n        decoded = gpt4_tokenizer.decode(tokens)\n        \n        # Check if roundtrip was successful\n        success = text == decoded\n        \n        print(f\"\\nText type: {name}\")\n        print(f\"Original: '{text}'\")\n        print(f\"Encoded: {tokens[:10]}{'...' if len(tokens) > 10 else ''} ({len(tokens)} tokens)\")\n        print(f\"Decoded: '{decoded}'\")\n        print(f\"Roundtrip success: {success}\")\n        \n        # If roundtrip failed, show where the difference is\n        if not success:\n            print(\"Difference:\")\n            for i, (a, b) in enumerate(zip(text, decoded)):\n                if a != b:\n                    print(f\"Position {i}: '{a}' vs '{b}'\")\n                    break\n    \n    except Exception as e:\n        print(f\"\\nText type: {name}\")\n        print(f\"Error: {str(e)}\")\n\n# Test byte shuffling specifically\nprint(\"\\nByte Shuffling Test:\")\nprint(\"=\" * 50)\n\n# Create text with bytes that would be shuffled\ntext_with_special_bytes = \"!@#$%^&*()_+{}|:\\\"<>?~`-=[]\\\\;',./\"\nencoded_shuffled = gpt4_tokenizer.encode(text_with_special_bytes)\ndecoded_shuffled = gpt4_tokenizer.decode(encoded_shuffled)\n\nprint(f\"Original: '{text_with_special_bytes}'\")\nprint(f\"Encoded: {encoded_shuffled}\")\nprint(f\"Decoded: '{decoded_shuffled}'\")\nprint(f\"Roundtrip success: {text_with_special_bytes == decoded_shuffled}\")\n\n# Visualization of token boundaries with special tokens\nprint(\"\\nToken Boundary Visualization:\")\nprint(\"=\" * 50)\n\nmixed_text = \"Hello<|endoftext|>World! Let's see 😊 how this works.\"\nencoded_mixed = gpt4_tokenizer.encode(mixed_text, allowed_special=\"all\")\n\n# Create a visualization with token boundaries\ntokens_vis = []\nfor token_id in encoded_mixed:\n    if token_id in gpt4_tokenizer.inverse_special_tokens:\n        # This is a special token\n        special_str = gpt4_tokenizer.inverse_special_tokens[token_id]\n        tokens_vis.append(f\"[*{special_str}*]\")\n    else:\n        # Regular token\n        token_bytes = gpt4_tokenizer.vocab[token_id]\n        try:\n            token_str = token_bytes.decode('utf-8')\n            tokens_vis.append(f\"[{token_str}]\")\n        except UnicodeDecodeError:\n            tokens_vis.append(f\"[hex: {token_bytes.hex()}]\")\n\nprint(f\"Text: '{mixed_text}'\")\nprint(f\"Tokenized: {''.join(tokens_vis)}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Optional: tiktoken Compatibility\n\nThe following demonstrates how we could extend our tokenizer to be compatible with tiktoken's encodings. This is optional and requires the tiktoken package to be installed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# This cell is commented out as it requires tiktoken to be installed\n# Uncomment and run if you have tiktoken available\n\n\"\"\"\ntry:\n    import tiktoken\n    \n    # Compare our tokenizer with tiktoken\n    def compare_with_tiktoken(text, encoding_name=\"cl100k_base\"):\n        # Get tiktoken encoding\n        enc = tiktoken.get_encoding(encoding_name)\n        tiktoken_tokens = enc.encode(text)\n        \n        # Use our GPT4Tokenizer\n        # In a full implementation, we would initialize our tokenizer\n        # with the exact same merges as tiktoken, but for this example\n        # we'll just show the concept\n        our_tokens = gpt4_tokenizer.encode(text, allowed_special=\"all\")\n        \n        print(f\"Text: '{text}'\")\n        print(f\"tiktoken ({encoding_name}): {tiktoken_tokens}\")\n        print(f\"Our GPT4Tokenizer: {our_tokens}\")\n        \n        # For a basic comparison, we can check if the token count is similar\n        print(f\"Token count comparison: tiktoken={len(tiktoken_tokens)}, ours={len(our_tokens)}\")\n        \n        return tiktoken_tokens, our_tokens\n    \n    # Try with different texts\n    test_samples = [\n        \"Hello world\",\n        \"The quick brown fox jumps over the lazy dog\",\n        \"def hello(): print('Hello, world!')\",\n        \"😊 🚀 💻\",\n        \"<|endoftext|>Special token test\"\n    ]\n    \n    for sample in test_samples:\n        print(\"\\n\" + \"=\"*50)\n        tiktoken_tokens, our_tokens = compare_with_tiktoken(sample)\n\nexcept ImportError:\n    print(\"tiktoken is not installed. Install with: pip install tiktoken\")\n\"\"\"",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Karpathy-Compatible Save/Load Functionality\n\nTokenizer persistence is crucial for practical applications. Our implementation follows Karpathy's minbpe format for compatibility:\n\n### File Format\n\n1. **`.model` File**: Contains all information needed to recreate the tokenizer\n   - Header: `minbpe v1`\n   - Regex pattern (if applicable)\n   - Special tokens count and mappings\n   - Merge pairs in the order they were learned\n\n2. **`.vocab` File**: Human-readable vocabulary for inspection\n   - Shows each token with its string representation\n   - For merged tokens, shows the constituent parts\n   - Includes formatting for whitespace and non-printable characters\n\n### Benefits of This Approach\n\n- **Compact Representation**: Only stores essential information (merges and special tokens)\n- **Human Readability**: The .vocab file allows for inspection and debugging\n- **Compatibility**: Works with Karpathy's original implementation\n- **Class Flexibility**: Works with any tokenizer class through polymorphism\n\nThe implementation handles all tokenizer variants including those with regex patterns, special tokens, and byte shuffling, ensuring consistent behavior after serialization and deserialization.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def save_tokenizer_karpathy_format(tokenizer, file_prefix):\n    \"\"\"\n    Save a tokenizer in Karpathy's format.\n    Creates two files:\n    - file_prefix.model: Contains the pattern, special tokens, and merges (used for loading)\n    - file_prefix.vocab: Human-readable vocabulary (for inspection only)\n    \n    Args:\n        tokenizer: The tokenizer to save\n        file_prefix: Path prefix for the saved files\n    \"\"\"\n    # Write the model file - used for loading later\n    model_file = file_prefix + \".model\"\n    with open(model_file, 'w', encoding='utf-8') as f:\n        # Write version and pattern\n        f.write(\"minbpe v1\\n\")\n        \n        # Write pattern if available\n        pattern = getattr(tokenizer, 'pattern', '')\n        f.write(f\"{pattern}\\n\")\n        \n        # Write special tokens\n        special_tokens = getattr(tokenizer, 'special_tokens', {})\n        f.write(f\"{len(special_tokens)}\\n\")\n        for special, idx in special_tokens.items():\n            f.write(f\"{special} {idx}\\n\")\n            \n        # Write the merges\n        for (idx1, idx2), idx in tokenizer.merges.items():\n            f.write(f\"{idx1} {idx2}\\n\")\n    \n    # Write the vocab file - for human inspection\n    vocab_file = file_prefix + \".vocab\"\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        # Build an inverted merges dictionary for visualization\n        inverted_merges = {idx: pair for pair, idx in tokenizer.merges.items()}\n        \n        # Write each token with its source if available\n        for idx, token in sorted(tokenizer.vocab.items()):\n            # Try to decode the token for display\n            try:\n                token_str = token.decode('utf-8', errors='replace')\n                token_str = token_str.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n                if len(token_str.strip()) == 0:\n                    # For whitespace, show hex\n                    token_str = f\"hex: {token.hex()}\"\n            except:\n                token_str = f\"hex: {token.hex()}\"\n                \n            # If this token has children, show the merge\n            if idx in inverted_merges:\n                idx0, idx1 = inverted_merges[idx]\n                \n                # Get string representations of the children\n                try:\n                    s0 = tokenizer.vocab[idx0].decode('utf-8', errors='replace')\n                    s0 = s0.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n                    if len(s0.strip()) == 0:\n                        s0 = f\"hex: {tokenizer.vocab[idx0].hex()}\"\n                except:\n                    s0 = f\"hex: {tokenizer.vocab[idx0].hex()}\"\n                    \n                try:\n                    s1 = tokenizer.vocab[idx1].decode('utf-8', errors='replace')\n                    s1 = s1.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n                    if len(s1.strip()) == 0:\n                        s1 = f\"hex: {tokenizer.vocab[idx1].hex()}\"\n                except:\n                    s1 = f\"hex: {tokenizer.vocab[idx1].hex()}\"\n                \n                # Write the merge information\n                f.write(f\"[{s0}][{s1}] -> [{token_str}] {idx}\\n\")\n            else:\n                # This is a leaf token (raw byte or special token)\n                f.write(f\"[{token_str}] {idx}\\n\")\n                \n    print(f\"Saved tokenizer to {model_file} and {vocab_file}\")\n\ndef load_tokenizer_karpathy_format(tokenizer_class, model_file):\n    \"\"\"\n    Load a tokenizer from a .model file in Karpathy's format.\n    \n    Args:\n        tokenizer_class: The tokenizer class to instantiate (e.g., Tokenizer, RegexTokenizer)\n        model_file: Path to the .model file\n        \n    Returns:\n        An instance of the tokenizer class loaded with the model\n    \"\"\"\n    assert model_file.endswith(\".model\"), \"File must have .model extension\"\n    \n    # Create a new tokenizer instance\n    tokenizer = tokenizer_class()\n    \n    # Read the model file\n    with open(model_file, 'r', encoding=\"utf-8\") as f:\n        # Read version\n        version = f.readline().strip()\n        assert version == \"minbpe v1\", f\"Unknown model version: {version}\"\n        \n        # Read pattern if available\n        pattern = f.readline().strip()\n        if hasattr(tokenizer, 'pattern'):\n            tokenizer.pattern = pattern\n            tokenizer.compiled_pattern = re.compile(pattern)\n        \n        # Read special tokens\n        num_special = int(f.readline().strip())\n        special_tokens = {}\n        for _ in range(num_special):\n            line = f.readline().strip()\n            special, special_idx = line.split(' ', 1)\n            special_tokens[special] = int(special_idx)\n        \n        # Read merges\n        tokenizer.merges = {}\n        next_idx = 256\n        for line in f:\n            try:\n                idx1, idx2 = map(int, line.strip().split())\n                tokenizer.merges[(idx1, idx2)] = next_idx\n                next_idx += 1\n            except:\n                # Skip malformed lines\n                continue\n    \n    # Rebuild vocab\n    tokenizer.vocab = {idx: bytes([idx]) for idx in range(256)}\n    for (p0, p1), idx in tokenizer.merges.items():\n        if p0 in tokenizer.vocab and p1 in tokenizer.vocab:\n            tokenizer.vocab[idx] = tokenizer.vocab[p0] + tokenizer.vocab[p1]\n    \n    # Set vocabulary size\n    tokenizer.vocab_size = max(tokenizer.vocab.keys()) + 1 if tokenizer.vocab else 256\n    \n    # Add special tokens if supported\n    if hasattr(tokenizer, 'register_special_tokens'):\n        tokenizer.register_special_tokens(special_tokens)\n    elif hasattr(tokenizer, 'special_tokens'):\n        tokenizer.special_tokens = special_tokens\n        tokenizer.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n        # Add special tokens to vocabulary\n        for token, idx in special_tokens.items():\n            tokenizer.vocab[idx] = token.encode('utf-8')\n    \n    # Update token_to_id for efficient encoding if applicable\n    if hasattr(tokenizer, 'token_to_id'):\n        tokenizer.token_to_id = {token: idx for idx, token in tokenizer.vocab.items()}\n    \n    print(f\"Loaded tokenizer from {model_file} with {len(tokenizer.merges)} merges\")\n    return tokenizer\n\n# Monkey patch the Tokenizer class and its subclasses to add these methods\nTokenizer.save_karpathy_format = save_tokenizer_karpathy_format\nTokenizer.load_karpathy_format = classmethod(lambda cls, model_file: load_tokenizer_karpathy_format(cls, model_file))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Testing Save/Load Functionality\n\nLet's test our Karpathy-compatible save and load functionality with different tokenizer types to ensure compatibility.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test the Karpathy-compatible save/load functionality across tokenizer types\nimport os\n\ndef test_save_load_karpathy_format(tokenizer, test_text, allowed_special=\"none\"):\n    \"\"\"Test save and load functionality for a tokenizer.\"\"\"\n    \n    # Get tokenizer name for the file prefix\n    tokenizer_name = tokenizer.__class__.__name__\n    file_prefix = f\"test_{tokenizer_name.lower()}\"\n    \n    # Save the tokenizer\n    tokenizer.save_karpathy_format(file_prefix)\n    \n    # Load the tokenizer using the class method\n    loaded_tokenizer = tokenizer.__class__.load_karpathy_format(f\"{file_prefix}.model\")\n    \n    # Encode test text with both tokenizers\n    original_tokens = tokenizer.encode(test_text, allowed_special=allowed_special) if hasattr(tokenizer, 'special_tokens') else tokenizer.encode(test_text)\n    loaded_tokens = loaded_tokenizer.encode(test_text, allowed_special=allowed_special) if hasattr(loaded_tokenizer, 'special_tokens') else loaded_tokenizer.encode(test_text)\n    \n    # Compare results\n    tokens_match = original_tokens == loaded_tokens\n    \n    # Check decoding\n    original_decoded = tokenizer.decode(original_tokens)\n    loaded_decoded = loaded_tokenizer.decode(loaded_tokens)\n    decoding_match = original_decoded == loaded_decoded\n    roundtrip_match = original_decoded == test_text\n    \n    # Print results\n    print(f\"Testing {tokenizer_name}:\")\n    print(f\"  Original tokens: {original_tokens[:10]}{'...' if len(original_tokens) > 10 else ''}\")\n    print(f\"  Loaded tokens:   {loaded_tokens[:10]}{'...' if len(loaded_tokens) > 10 else ''}\")\n    print(f\"  Tokens match: {tokens_match}\")\n    print(f\"  Decoding match: {decoding_match}\")\n    print(f\"  Roundtrip match: {roundtrip_match}\")\n    \n    # Check vocabulary size\n    print(f\"  Original vocab size: {tokenizer.vocab_size}\")\n    print(f\"  Loaded vocab size: {loaded_tokenizer.vocab_size}\")\n    \n    # Check any special attributes\n    for attr in ['pattern', 'special_tokens']:\n        if hasattr(tokenizer, attr):\n            original_value = getattr(tokenizer, attr)\n            loaded_value = getattr(loaded_tokenizer, attr)\n            print(f\"  {attr} match: {original_value == loaded_value}\")\n    \n    # Clean up test files\n    for ext in ['.model', '.vocab']:\n        file_path = f\"{file_prefix}{ext}\"\n        if os.path.exists(file_path):\n            os.remove(file_path)\n            \n    return tokens_match and decoding_match\n\n# Test text with special characters and multilingual content\ntest_text = \"\"\"\nThis is a test of the save/load functionality.\nIt includes special characters: !@#$%^&*()_+\nAnd multilingual content: こんにちは, Привет, مرحبا, 你好\nAs well as emojis: 😊 🚀 💻\n\"\"\"\n\n# Test with special tokens\nspecial_test_text = \"Hello <|endoftext|> world <|fim_prefix|> test\"\n\n# Test the basic tokenizer\nprint(\"\\nTesting Basic Tokenizer:\")\nbasic_tokenizer = Tokenizer()\nbasic_tokenizer.train(test_text, vocab_size=300, verbose=False)\ntest_save_load_karpathy_format(basic_tokenizer, test_text)\n\n# Test the regex tokenizer\nprint(\"\\nTesting Regex Tokenizer:\")\nregex_tokenizer = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\nregex_tokenizer.train(test_text, vocab_size=300, verbose=False)\ntest_save_load_karpathy_format(regex_tokenizer, test_text)\n\n# Test the special tokens tokenizer\nprint(\"\\nTesting Special Tokens Tokenizer:\")\nspecial_tokenizer = SpecialTokensTokenizer(pattern=GPT4_SPLIT_PATTERN)\nspecial_tokenizer.train(test_text, vocab_size=300, verbose=False)\nspecial_tokenizer.register_special_tokens({\n    '<|endoftext|>': 100257,\n    '<|fim_prefix|>': 100258\n})\ntest_save_load_karpathy_format(special_tokenizer, special_test_text, allowed_special=\"all\")\n\n# Test the GPT4 tokenizer\nprint(\"\\nTesting GPT4 Tokenizer:\")\ngpt4_tokenizer = GPT4Tokenizer()\ngpt4_tokenizer.train(test_text, vocab_size=300, verbose=False)\ntest_save_load_karpathy_format(gpt4_tokenizer, special_test_text, allowed_special=\"all\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Comprehensive Performance Benchmarking\n\nTo understand the real-world performance of our tokenizer implementations, let's conduct more thorough benchmarking across a variety of text types and compare the different tokenizer classes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef benchmark_tokenizers(test_cases, tokenizers):\n    \"\"\"\n    Benchmark different tokenizer implementations across various test cases.\n    \n    Args:\n        test_cases: Dictionary mapping test case names to text samples\n        tokenizers: Dictionary mapping tokenizer names to tokenizer instances\n        \n    Returns:\n        Dictionary of benchmark results\n    \"\"\"\n    results = {}\n    \n    # For each tokenizer\n    for tokenizer_name, tokenizer in tokenizers.items():\n        tokenizer_results = {}\n        \n        # For each test case\n        for case_name, text in test_cases.items():\n            # Special token handling for applicable tokenizers\n            allowed_special = \"all\" if hasattr(tokenizer, 'special_tokens') else None\n            \n            # Benchmark encoding\n            start_time = time.time()\n            tokens = tokenizer.encode(text, allowed_special=allowed_special) if allowed_special else tokenizer.encode(text)\n            encode_time = time.time() - start_time\n            \n            # Benchmark decoding\n            start_time = time.time()\n            decoded = tokenizer.decode(tokens)\n            decode_time = time.time() - start_time\n            \n            # Calculate metrics\n            num_chars = len(text)\n            num_tokens = len(tokens)\n            chars_per_token = num_chars / num_tokens if num_tokens > 0 else 0\n            tokens_per_second = num_tokens / encode_time if encode_time > 0 else 0\n            chars_per_second = num_chars / encode_time if encode_time > 0 else 0\n            roundtrip_success = text == decoded\n            \n            # Store results\n            tokenizer_results[case_name] = {\n                \"chars\": num_chars,\n                \"tokens\": num_tokens,\n                \"encode_time\": encode_time,\n                \"decode_time\": decode_time,\n                \"chars_per_token\": chars_per_token,\n                \"tokens_per_second\": tokens_per_second,\n                \"chars_per_second\": chars_per_second,\n                \"roundtrip_success\": roundtrip_success\n            }\n            \n        results[tokenizer_name] = tokenizer_results\n    \n    return results\n\ndef plot_benchmark_results(results):\n    \"\"\"Plot benchmark results for visual comparison.\"\"\"\n    tokenizer_names = list(results.keys())\n    case_names = list(results[tokenizer_names[0]].keys())\n    metrics = [\"tokens_per_second\", \"chars_per_token\", \"chars_per_second\"]\n    \n    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 12))\n    \n    for i, metric in enumerate(metrics):\n        ax = axes[i]\n        \n        # Prepare data for plotting\n        x = np.arange(len(case_names))\n        width = 0.8 / len(tokenizer_names)\n        \n        # Plot bars for each tokenizer\n        for j, tokenizer_name in enumerate(tokenizer_names):\n            metric_values = [results[tokenizer_name][case][metric] for case in case_names]\n            offset = j * width - (len(tokenizer_names) - 1) * width / 2\n            ax.bar(x + offset, metric_values, width, label=tokenizer_name)\n        \n        # Set labels and title\n        ax.set_ylabel(metric.replace('_', ' ').title())\n        ax.set_title(f\"{metric.replace('_', ' ').title()} by Test Case\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(case_names, rotation=45, ha='right')\n        ax.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# Define comprehensive test cases\ncomprehensive_test_cases = {\n    \"English\": \"The quick brown fox jumps over the lazy dog. This sentence contains all letters in the English alphabet.\",\n    \"Code\": \"def fibonacci(n):\\n    a, b = 0, 1\\n    for _ in range(n):\\n        a, b = b, a + b\\n    return a\\n\\nprint(fibonacci(10))\",\n    \"JSON\": \"\"\"{\"name\": \"John Doe\", \"age\": 30, \"isActive\": true, \"hobbies\": [\"reading\", \"swimming\", \"cycling\"]}\"\"\",\n    \"Emoji\": \"I love coding! 😊 💻 🚀 🔥 👍 🎉 💡 📱 🌎 🔍\",\n    \"Multilingual\": \"Hello (English), Bonjour (French), こんにちは (Japanese), 你好 (Chinese), مرحبا (Arabic), Привет (Russian)\",\n    \"Numbers\": \"0123456789 3.14159 2.71828 1234567890 9876543210 42 7 365 24 60 60\",\n    \"Special\": \"<|endoftext|>This contains special tokens<|fim_prefix|>that need<|fim_middle|>special handling<|fim_suffix|>\",\n    \"Mixed\": \"This is a mixed text with code: `print('hello')`, numbers: 12345, and emoji: 😊!\"\n}\n\n# Create instances of each tokenizer type using the same training text\ncombined_training_text = \" \".join(comprehensive_test_cases.values())\n\ntokenizer_instances = {}\n\n# Basic tokenizer\nbasic_tokenizer = Tokenizer()\nbasic_tokenizer.train(combined_training_text, vocab_size=500, verbose=False)\ntokenizer_instances[\"Basic\"] = basic_tokenizer\n\n# Regex tokenizer\nregex_tokenizer = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\nregex_tokenizer.train(combined_training_text, vocab_size=500, verbose=False)\ntokenizer_instances[\"Regex\"] = regex_tokenizer\n\n# Special tokens tokenizer\nspecial_tokenizer = SpecialTokensTokenizer(pattern=GPT4_SPLIT_PATTERN)\nspecial_tokenizer.train(combined_training_text, vocab_size=500, verbose=False)\nspecial_tokenizer.register_special_tokens({\n    '<|endoftext|>': 100257,\n    '<|fim_prefix|>': 100258,\n    '<|fim_middle|>': 100259,\n    '<|fim_suffix|>': 100260\n})\ntokenizer_instances[\"SpecialTokens\"] = special_tokenizer\n\n# GPT4 tokenizer\ngpt4_tokenizer = GPT4Tokenizer()\ngpt4_tokenizer.train(combined_training_text, vocab_size=500, verbose=False)\ntokenizer_instances[\"GPT4\"] = gpt4_tokenizer\n\n# Run the benchmarks\nbenchmark_results = benchmark_tokenizers(comprehensive_test_cases, tokenizer_instances)\n\n# Display results in tabular format\nheaders = [\"Tokenizer\", \"Test Case\", \"Chars\", \"Tokens\", \"Chars/Token\", \"Tokens/sec\", \"Chars/sec\", \"Success\"]\nrows = []\n\nfor tokenizer_name, test_cases in benchmark_results.items():\n    for case_name, metrics in test_cases.items():\n        rows.append([\n            tokenizer_name,\n            case_name,\n            metrics[\"chars\"],\n            metrics[\"tokens\"],\n            f\"{metrics['chars_per_token']:.2f}\",\n            f\"{metrics['tokens_per_second']:.2f}\",\n            f\"{metrics['chars_per_second']:.2f}\",\n            \"✓\" if metrics[\"roundtrip_success\"] else \"✗\"\n        ])\n\n# Print the table\nprint(f\"{headers[0]:<15} | {headers[1]:<15} | {headers[2]:<8} | {headers[3]:<8} | \"\n      f\"{headers[4]:<12} | {headers[5]:<12} | {headers[6]:<12} | {headers[7]}\")\nprint(\"-\" * 90)\n\nfor row in rows:\n    print(f\"{row[0]:<15} | {row[1]:<15} | {row[2]:<8} | {row[3]:<8} | \"\n          f\"{row[4]:<12} | {row[5]:<12} | {row[6]:<12} | {row[7]}\")\n\n# Plot the results\nplot_benchmark_results(benchmark_results)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Edge Case Testing for Unicode and Multilingual Content\n\nTokenizers often face challenges with edge cases, particularly around Unicode handling and multilingual text. Let's test our implementation against various challenging scenarios to ensure robustness.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def test_edge_cases(tokenizer, test_cases, verbose=True):\n    \"\"\"\n    Test a tokenizer against edge cases, focusing on robust handling of challenging inputs.\n    \n    Args:\n        tokenizer: The tokenizer to test\n        test_cases: Dictionary mapping test case names to text samples\n        verbose: Whether to print detailed results\n        \n    Returns:\n        Dictionary of results for each test case\n    \"\"\"\n    results = {}\n    \n    for case_name, text in test_cases.items():\n        if verbose:\n            print(f\"\\n== Testing: {case_name} ==\")\n            print(f\"Input: '{text}'\")\n        \n        # Handle special tokens if applicable\n        allowed_special = \"all\" if hasattr(tokenizer, 'special_tokens') else None\n        \n        try:\n            # Try to encode\n            tokens = tokenizer.encode(text, allowed_special=allowed_special) if allowed_special else tokenizer.encode(text)\n            \n            if verbose:\n                print(f\"Encoded: {tokens}\")\n            \n            # Try to decode\n            decoded = tokenizer.decode(tokens)\n            \n            if verbose:\n                print(f\"Decoded: '{decoded}'\")\n            \n            # Check roundtrip success\n            success = text == decoded\n            \n            if verbose:\n                print(f\"Roundtrip success: {success}\")\n                \n                # If failed, show difference\n                if not success:\n                    print(\"Difference:\")\n                    for i, (a, b) in enumerate(zip(text, decoded)):\n                        if a != b:\n                            print(f\"  Position {i}: '{a}' (U+{ord(a):04X}) vs '{b}' (U+{ord(b):04X})\")\n                            break\n                    \n                    # Check if lengths differ\n                    if len(text) != len(decoded):\n                        print(f\"  Length mismatch: original={len(text)}, decoded={len(decoded)}\")\n            \n            results[case_name] = {\n                \"success\": success,\n                \"tokens\": tokens,\n                \"decoded\": decoded,\n                \"error\": None\n            }\n            \n        except Exception as e:\n            if verbose:\n                print(f\"Error: {str(e)}\")\n            \n            results[case_name] = {\n                \"success\": False,\n                \"tokens\": None,\n                \"decoded\": None,\n                \"error\": str(e)\n            }\n    \n    # Print summary\n    successes = sum(1 for r in results.values() if r[\"success\"])\n    \n    if verbose:\n        print(f\"\\nSummary: {successes}/{len(test_cases)} tests passed\")\n    \n    return results\n\n# Define comprehensive edge cases\nunicode_edge_cases = {\n    \"Zero-width characters\": \"Text with zero-width joiner: a\\u200Db and zero-width non-joiner: a\\u200Cb\",\n    \"Combining marks\": \"Combining diacritical marks: e\\u0301 a\\u0300 o\\u0308\",\n    \"RTL text\": \"Right-to-left text with control chars: \\u2067Hello\\u2069 \\u2066العربية\\u2069\",\n    \"Surrogate pairs\": \"Surrogate pairs: \\ud83d\\ude00 \\ud83d\\ude42 \\ud83d\\ude1c\",\n    \"Rare Unicode\": \"Rare characters: \\u16A0 \\u16A1 \\u16A2 (Runic), \\u1100 \\u1101 \\u1102 (Hangul)\",\n    \"ASCII control chars\": \"ASCII control: \\u0001\\u0002\\u0003\\u0004\\u0005\\u0006\\u0007\",\n    \"Emoji sequencing\": \"👨‍👩‍👧‍👦 👨‍👨‍👧‍👧 👩‍👩‍👧‍👦 (family emojis with ZWJ sequences)\",\n    \"Emoji modifiers\": \"👋🏻 👋🏼 👋🏽 👋🏾 👋🏿 (hand with skin tone modifiers)\",\n    \"Language mixing\": \"Mixed: English, Русский, 中文, العربية, हिन्दी in one string\",\n    \"Special tokens within Unicode\": \"Before<|endoftext|>اللغة العربية<|fim_prefix|>中文\",\n    \"Very long string\": \"a\" * 1000 + \"😊\" * 100 + \"b\" * 1000,\n    \"Empty string\": \"\",\n    \"Boundary chars\": \"\\u0000\\u0001\\uFFFF\\uFFFE\",\n    \"Bidirectional text\": \"Mixed English and العربية with ⁧Arabic numerals ١٢٣⁩\",\n    \"Invalid UTF-8 replacement\": \"This text has invalid bytes that would be replacement chars in UTF-8 decoding\"\n}\n\n# Test with our most advanced tokenizer\nprint(\"Testing GPT4Tokenizer against Unicode edge cases:\")\nedge_case_results = test_edge_cases(gpt4_tokenizer, unicode_edge_cases)\n\n# Evaluate which categories cause the most issues\nissue_categories = [case for case, result in edge_case_results.items() if not result[\"success\"]]\n\nif issue_categories:\n    print(\"\\nCategories with issues:\")\n    for category in issue_categories:\n        print(f\"- {category}\")\n        \n    print(\"\\nPotential improvement areas:\")\n    if any(\"RTL\" in cat or \"Bidirectional\" in cat for cat in issue_categories):\n        print(\"- Improve bidirectional text handling\")\n    if any(\"surrogate\" in cat.lower() or \"emoji\" in cat.lower() for cat in issue_categories):\n        print(\"- Enhance emoji and surrogate pair handling\")\n    if any(\"special\" in cat.lower() for cat in issue_categories):\n        print(\"- Review special token interaction with Unicode\")\nelse:\n    print(\"\\nAll edge cases passed successfully!\")",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BPE Tokenizer",
   "language": "python",
   "name": "bpe_tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}