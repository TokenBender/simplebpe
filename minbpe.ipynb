{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minbpe: Minimal Byte Pair Encoding Tokenizer\n",
    "\n",
    "This notebook contains a faithful implementation of Andrej Karpathy's minbpe tokenizer, emphasizing:\n",
    "- Clean, minimal code\n",
    "- Byte-level tokenization\n",
    "- Educational clarity\n",
    "\n",
    "The implementation follows the core design philosophy of Karpathy's approach, preserving the simplicity and readability of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set, Any, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Tokenizer Implementation\n",
    "\n",
    "We start with a basic tokenizer class that implements the core BPE algorithm without any regex-based preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"A minimal Byte Pair Encoding tokenizer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize with the base 256 tokens (raw bytes 0-255)\n",
    "        self.merges = {}  # (token1, token2) -> new_token_id \n",
    "        self.vocab = {}   # token_id -> token (bytes)\n",
    "        self.vocab_size = 0\n",
    "        self.special_tokens = {}\n",
    "        \n",
    "        # Pre-populate the vocabulary with the basic 256 byte tokens\n",
    "        for i in range(256):\n",
    "            token = bytes([i])\n",
    "            self.vocab[i] = token\n",
    "            \n",
    "        self.vocab_size = 256\n",
    "    \n",
    "    def train(self, text: str, vocab_size: int, verbose: bool = False) -> None:\n",
    "        \"\"\"Train the tokenizer on text, extending the vocabulary to the desired size.\"\"\"\n",
    "        # Convert text to bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        # Keep track of progress\n",
    "        if verbose:\n",
    "            print(f\"Training BPE tokenizer to vocab size {vocab_size}\")\n",
    "            print(f\"Text size: {len(text)} chars, {len(ids)} bytes\")\n",
    "        \n",
    "        # Iteratively merge the most frequent pair until we reach the desired vocab size\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            # Count frequencies of adjacent pairs\n",
    "            stats = self.get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "                \n",
    "            # Find the most frequent pair\n",
    "            pair = max(stats, key=stats.get)\n",
    "            \n",
    "            # Create a new token for this pair\n",
    "            token1, token2 = pair\n",
    "            new_token = self.vocab[token1] + self.vocab[token2]\n",
    "            new_id = self.vocab_size\n",
    "            \n",
    "            # Add merge to our vocabulary\n",
    "            self.merges[pair] = new_id\n",
    "            self.vocab[new_id] = new_token\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "            # Apply the merge to the current token list\n",
    "            ids = self.merge(ids, pair, new_id)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"Merge #{i}: pair {pair} -> {new_id}, corpus now {len(ids)} tokens\")\n",
    "    \n",
    "    def get_stats(self, ids: List[int]) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Count the frequencies of adjacent token pairs.\"\"\"\n",
    "        stats = Counter()\n",
    "        for i in range(len(ids) - 1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            stats[pair] += 1\n",
    "        return stats\n",
    "    \n",
    "    def merge(self, ids: List[int], pair: Tuple[int, int], new_id: int) -> List[int]:\n",
    "        \"\"\"Replace all occurrences of a token pair with a new token ID.\"\"\"\n",
    "        # Create a new list for the merged result\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            # If we're at the last token, just add it\n",
    "            if i == len(ids) - 1:\n",
    "                new_ids.append(ids[i])\n",
    "                break\n",
    "            \n",
    "            # If current pair matches, merge and add the new token\n",
    "            if ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                new_ids.append(new_id)\n",
    "                i += 2  # Skip both tokens\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1  # Move to next token\n",
    "        \n",
    "        return new_ids\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        # Convert text to bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        # Apply merges iteratively, in the order they were learned\n",
    "        while len(ids) >= 2:\n",
    "            # Find valid merge pairs in the current sequence\n",
    "            pairs = [(ids[i], ids[i+1]) for i in range(len(ids)-1)]\n",
    "            valid_pairs = [(pair, self.merges[pair]) for pair in pairs if pair in self.merges]\n",
    "            \n",
    "            # If no valid pairs, we're done\n",
    "            if not valid_pairs:\n",
    "                break\n",
    "                \n",
    "            # Find the pair with the lowest merge ID (first learned)\n",
    "            pair, new_id = min(valid_pairs, key=lambda x: x[1])\n",
    "            \n",
    "            # Apply the merge\n",
    "            ids = self.merge(ids, pair, new_id)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        # Convert token IDs to bytes\n",
    "        bytes_list = []\n",
    "        for token_id in ids:\n",
    "            bytes_list.extend(self.vocab[token_id])\n",
    "        \n",
    "        # Convert bytes to UTF-8 text\n",
    "        text = bytes(bytes_list).decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def save(self, file_path: str) -> None:\n",
    "        \"\"\"Save the tokenizer to a file.\"\"\"\n",
    "        # Prepare model data - convert bytes to lists for JSON serialization\n",
    "        model_data = {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"merges\": {f\"{t1},{t2}\": idx for (t1, t2), idx in self.merges.items()},\n",
    "            \"vocab\": {str(i): list(t) for i, t in self.vocab.items() if i >= 256},\n",
    "            \"special_tokens\": self.special_tokens\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, file_path: str) -> None:\n",
    "        \"\"\"Load a tokenizer from a file.\"\"\"\n",
    "        # Read the model data\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        # Reset the tokenizer\n",
    "        self.__init__()\n",
    "        \n",
    "        # Load the vocabulary\n",
    "        self.vocab_size = model_data[\"vocab_size\"]\n",
    "        \n",
    "        # Add vocabulary items (skipping the base 256 bytes already initialized)\n",
    "        for token_id_str, token_bytes in model_data[\"vocab\"].items():\n",
    "            token_id = int(token_id_str)\n",
    "            self.vocab[token_id] = bytes(token_bytes)\n",
    "        \n",
    "        # Load merges\n",
    "        for pair_str, idx in model_data[\"merges\"].items():\n",
    "            t1, t2 = map(int, pair_str.split(\",\"))\n",
    "            self.merges[(t1, t2)] = idx\n",
    "        \n",
    "        # Load special tokens\n",
    "        self.special_tokens = model_data.get(\"special_tokens\", {})\n",
    "    \n",
    "    def token_to_str(self, token_id: int) -> str:\n",
    "        \"\"\"Get a string representation of a token for visualization.\"\"\"\n",
    "        token_bytes = self.vocab[token_id]\n",
    "        # Try to convert to UTF-8 string if possible\n",
    "        try:\n",
    "            s = token_bytes.decode('utf-8')\n",
    "            # Replace newlines, tabs, etc. for display\n",
    "            s = s.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            if len(s.strip()) == 0:\n",
    "                # If it's all whitespace, show hex\n",
    "                return f\"[hex: {token_bytes.hex()}]\"\n",
    "            return s\n",
    "        except UnicodeDecodeError:\n",
    "            # If not a valid UTF-8 string, show hex\n",
    "            return f\"[hex: {token_bytes.hex()}]\"\n",
    "    \n",
    "    def print_vocab(self, n=50) -> None:\n",
    "        \"\"\"Print the first n tokens in the vocabulary for inspection.\"\"\"\n",
    "        ids = sorted(self.vocab.keys())\n",
    "        skipped = max(0, len(ids) - n)\n",
    "        print(f\"Vocabulary size: {len(ids)} tokens\")\n",
    "        print(f\"Showing first {min(n, len(ids))} tokens:\")\n",
    "        for i, token_id in enumerate(ids[:n]):\n",
    "            s = self.token_to_str(token_id)\n",
    "            print(f\"Token {token_id}: {s}\")\n",
    "        if skipped > 0:\n",
    "            print(f\"... and {skipped} more tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tokenizer\n",
    "\n",
    "Let's test our implementation with a simple example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer to vocab size 500\n",
      "Text size: 656 chars, 662 bytes\n",
      "Merge #0: pair (101, 32) -> 256, corpus now 647 tokens\n",
      "Merge #100: pair (355, 110) -> 356, corpus now 311 tokens\n",
      "Merge #200: pair (455, 277) -> 456, corpus now 211 tokens\n",
      "Vocabulary size: 500 tokens\n",
      "Showing first 30 tokens:\n",
      "Token 0: \u0000\n",
      "Token 1: \u0001\n",
      "Token 2: \u0002\n",
      "Token 3: \u0003\n",
      "Token 4: \u0004\n",
      "Token 5: \u0005\n",
      "Token 6: \u0006\n",
      "Token 7: \u0007\n",
      "Token 8:\n",
      "Token 9: \\t\n",
      "Token 10: \\n\n",
      "Token 11: [hex: 0b]\n",
      "Token 12: [hex: 0c]\n",
      "Token 13: [hex: 0d]\n",
      "Token 14: \u000e\n",
      "Token 15: \u000f\n",
      "Token 16: \u0010\n",
      "Token 17: \u0011\n",
      "Token 18: \u0012\n",
      "Token 19: \u0013\n",
      "Token 20: \u0014\n",
      "Token 21: \u0015\n",
      "Token 22: \u0016\n",
      "Token 23: \u0017\n",
      "Token 24: \u0018\n",
      "Token 25: \u0019\n",
      "Token 26: \u001a\n",
      "Token 27: \u001b\n",
      "Token 28: [hex: 1c]\n",
      "Token 29: [hex: 1d]\n",
      "... and 470 more tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a simple training corpus\n",
    "training_text = \"\"\"\n",
    "Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of consecutive bytes in a sequence with a single, unused byte. In NLP, it is used as a subword tokenization algorithm.\n",
    "\n",
    "The BPE algorithm works as follows:\n",
    "1. Initialize the vocabulary with individual characters/bytes\n",
    "2. Count all pairs of adjacent symbols in the training corpus\n",
    "3. Merge the most frequent pair and add it to the vocabulary\n",
    "4. Repeat steps 2-3 until reaching the desired vocabulary size\n",
    "\n",
    "BPE can handle out-of-vocabulary words by splitting them into known subword units, making it effective for various languages and even emoji üëçüåç.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize our tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Train to a vocabulary size of 500\n",
    "tokenizer.train(training_text, vocab_size=500, verbose=True)\n",
    "\n",
    "# Show some of the learned tokens\n",
    "tokenizer.print_vocab(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "Now let's test encoding and decoding to verify the tokenizer works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded into 42 tokens: [292, 32, 305, 107, 269, 306, 97, 266, 111, 293, 282, 107, 257, 103, 265, 311, 102, 262, 32, 110, 97, 116, 117, 295, 108, 32, 267, 110, 103, 117, 97, 103, 256, 112, 114, 111, 99, 101, 115, 115, 264, 33]\n",
      "\n",
      "Token breakdown:\n",
      "Token 1: ID 292 = 'BPE'\n",
      "Token 2: ID 32 = '[hex: 20]'\n",
      "Token 3: ID 305 = 'to'\n",
      "Token 4: ID 107 = 'k'\n",
      "Token 5: ID 269 = 'en'\n",
      "Token 6: ID 306 = 'iz'\n",
      "Token 7: ID 97 = 'a'\n",
      "Token 8: ID 266 = 'ti'\n",
      "Token 9: ID 111 = 'o'\n",
      "Token 10: ID 293 = 'n '\n",
      "Token 11: ID 282 = 'wor'\n",
      "Token 12: ID 107 = 'k'\n",
      "Token 13: ID 257 = 's '\n",
      "Token 14: ID 103 = 'g'\n",
      "Token 15: ID 265 = 're'\n",
      "Token 16: ID 311 = 'at '\n",
      "Token 17: ID 102 = 'f'\n",
      "Token 18: ID 262 = 'or'\n",
      "Token 19: ID 32 = '[hex: 20]'\n",
      "Token 20: ID 110 = 'n'\n",
      "Token 21: ID 97 = 'a'\n",
      "Token 22: ID 116 = 't'\n",
      "Token 23: ID 117 = 'u'\n",
      "Token 24: ID 295 = 'ra'\n",
      "Token 25: ID 108 = 'l'\n",
      "Token 26: ID 32 = '[hex: 20]'\n",
      "Token 27: ID 267 = 'la'\n",
      "Token 28: ID 110 = 'n'\n",
      "Token 29: ID 103 = 'g'\n",
      "Token 30: ID 117 = 'u'\n",
      "Token 31: ID 97 = 'a'\n",
      "Token 32: ID 103 = 'g'\n",
      "Token 33: ID 256 = 'e '\n",
      "Token 34: ID 112 = 'p'\n",
      "Token 35: ID 114 = 'r'\n",
      "Token 36: ID 111 = 'o'\n",
      "Token 37: ID 99 = 'c'\n",
      "Token 38: ID 101 = 'e'\n",
      "Token 39: ID 115 = 's'\n",
      "Token 40: ID 115 = 's'\n",
      "Token 41: ID 264 = 'ing'\n",
      "Token 42: ID 33 = '!'\n",
      "\n",
      "Decoded text: 'BPE tokenization works great for natural language processing!'\n",
      "Round trip success: True\n"
     ]
    }
   ],
   "source": [
    "# Test with a new sentence\n",
    "test_text = \"BPE tokenization works great for natural language processing!\"\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"Encoded into {len(encoded)} tokens: {encoded}\")\n",
    "\n",
    "# Display each token\n",
    "print(\"\\nToken breakdown:\")\n",
    "for i, token_id in enumerate(encoded):\n",
    "    print(f\"Token {i+1}: ID {token_id} = '{tokenizer.token_to_str(token_id)}'\")\n",
    "\n",
    "# Decode the tokens back to text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nDecoded text: '{decoded}'\")\n",
    "print(f\"Round trip success: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Tokenization Efficiency\n",
    "\n",
    "Let's compute some metrics on the tokenization efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text            | Chars    | Tokens   | Ratio   | Encode (s) | Decode (s) | Success\n",
      "---------------------------------------------------------------------------\n",
      "English         | 44       | 35       | 1.26    | 0.0001     | 0.0000     | True\n",
      "Repeated        | 41       | 41       | 1.00    | 0.0000     | 0.0000     | True\n",
      "Numbers         | 32       | 32       | 1.00    | 0.0000     | 0.0000     | True\n",
      "Technical       | 60       | 50       | 1.20    | 0.0001     | 0.0000     | True\n",
      "Emoji           | 15       | 37       | 0.41    | 0.0000     | 0.0000     | True\n",
      "Mixed           | 35       | 48       | 0.73    | 0.0001     | 0.0000     | True\n"
     ]
    }
   ],
   "source": [
    "def measure_efficiency(tokenizer, texts):\n",
    "    \"\"\"Measure tokenization efficiency across multiple text samples.\"\"\"\n",
    "    results = []\n",
    "    for name, text in texts.items():\n",
    "        # Tokenize and measure\n",
    "        start_time = time.time()\n",
    "        tokens = tokenizer.encode(text)\n",
    "        encode_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        decode_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_count = len(text)\n",
    "        token_count = len(tokens)\n",
    "        compression_ratio = char_count / token_count\n",
    "        chars_per_second = char_count / encode_time if encode_time > 0 else 0\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"name\": name,\n",
    "            \"chars\": char_count,\n",
    "            \"tokens\": token_count,\n",
    "            \"ratio\": compression_ratio,\n",
    "            \"encode_time\": encode_time,\n",
    "            \"decode_time\": decode_time,\n",
    "            \"chars_per_second\": chars_per_second,\n",
    "            \"roundtrip_success\": text == decoded\n",
    "        })\n",
    "    \n",
    "    # Print results table\n",
    "    print(f\"{'Text':<15} | {'Chars':<8} | {'Tokens':<8} | {'Ratio':<7} | {'Encode (s)':<10} | {'Decode (s)':<10} | {'Success':<7}\")\n",
    "    print(\"-\" * 75)\n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<15} | {r['chars']:<8} | {r['tokens']:<8} | {r['ratio']:<7.2f} | {r['encode_time']:<10.4f} | {r['decode_time']:<10.4f} | {r['roundtrip_success']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test texts\n",
    "test_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Repeated\": \"hello hello hello hello hello hello hello\",\n",
    "    \"Numbers\": \"1234567890 1234567890 1234567890\",\n",
    "    \"Technical\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Emoji\": \"üôÇ üåç üöÄ üë®‚Äçüë©‚Äçüëß‚Äçüë¶ üéâ\",\n",
    "    \"Mixed\": \"Training at 3.5x speed: üòä Âø´ÈÄüËÆ≠ÁªÉÔºÅÈÄüÂ∫¶ÊèêÈ´ò\"\n",
    "}\n",
    "\n",
    "# Measure tokenization efficiency\n",
    "efficiency_results = measure_efficiency(tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Tokenization Process\n",
    "\n",
    "Let's create a visualization of how text gets split into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized into 34 tokens:\n",
      "[H][e][l][l][o][,‚ê£][wor][l][d][!][‚ê£][T][h][i][s‚ê£][i][s‚ê£a][‚ê£][te][s][t‚ê£][of][‚ê£][BPE][‚ê£][to][k][en][iz][a][ti][o][n][.]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 72 = 'H'\n",
      "Token 2: ID 101 = 'e'\n",
      "Token 3: ID 108 = 'l'\n",
      "Token 4: ID 108 = 'l'\n",
      "Token 5: ID 111 = 'o'\n",
      "Token 6: ID 303 = ',‚ê£'\n",
      "Token 7: ID 282 = 'wor'\n",
      "Token 8: ID 108 = 'l'\n",
      "Token 9: ID 100 = 'd'\n",
      "Token 10: ID 33 = '!'\n",
      "Token 11: ID 32 = '‚ê£'\n",
      "Token 12: ID 84 = 'T'\n",
      "Token 13: ID 104 = 'h'\n",
      "Token 14: ID 105 = 'i'\n",
      "Token 15: ID 257 = 's‚ê£'\n",
      "Token 16: ID 105 = 'i'\n",
      "Token 17: ID 277 = 's‚ê£a'\n",
      "Token 18: ID 32 = '‚ê£'\n",
      "Token 19: ID 263 = 'te'\n",
      "Token 20: ID 115 = 's'\n",
      "Token 21: ID 260 = 't‚ê£'\n",
      "Token 22: ID 300 = 'of'\n",
      "Token 23: ID 32 = '‚ê£'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '‚ê£'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 110 = 'n'\n",
      "Token 34: ID 46 = '.'\n"
     ]
    }
   ],
   "source": [
    "def visualize_tokenization(tokenizer, text):\n",
    "    \"\"\"Visualize how text is tokenized by showing token boundaries.\"\"\"\n",
    "    # Encode the text\n",
    "    ids = tokenizer.encode(text)\n",
    "    \n",
    "    # Get the bytes for each token\n",
    "    token_bytes = [tokenizer.vocab[id] for id in ids]\n",
    "    \n",
    "    # Try to display each token as text\n",
    "    visualized = []\n",
    "    for token in token_bytes:\n",
    "        try:\n",
    "            token_text = token.decode('utf-8')\n",
    "            # Replace whitespace for visibility\n",
    "            token_text = token_text.replace(' ', '‚ê£').replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            visualized.append(token_text)\n",
    "        except UnicodeDecodeError:\n",
    "            # If not a valid UTF-8 sequence, show hex\n",
    "            visualized.append(f\"[{token.hex()}]\")\n",
    "    \n",
    "    # Display with token boundaries\n",
    "    print(f\"Tokenized into {len(ids)} tokens:\")\n",
    "    result = \"\"\n",
    "    for token in visualized:\n",
    "        result += f\"[{token}]\"\n",
    "    print(result)\n",
    "    \n",
    "    # Display each token with its ID\n",
    "    print(\"\\nDetailed token breakdown:\")\n",
    "    for i, (id, vis) in enumerate(zip(ids, visualized)):\n",
    "        print(f\"Token {i+1}: ID {id} = '{vis}'\")\n",
    "    \n",
    "    return visualized\n",
    "\n",
    "# Visualize tokenization for a sample text\n",
    "sample_text = \"Hello, world! This is a test of BPE tokenization.\"\n",
    "tokens_visualized = visualize_tokenization(tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Tokenizer\n",
    "\n",
    "Let's test the serialization and deserialization of our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer with 500 tokens\n",
      "Loaded tokenizer with 500 tokens\n",
      "Original tokenizer: 32 tokens\n",
      "Loaded tokenizer: 32 tokens\n",
      "Tokens match: True\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "print(f\"Saved tokenizer with {tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Create a new tokenizer and load the saved model\n",
    "new_tokenizer = Tokenizer()\n",
    "new_tokenizer.load(\"bpe_tokenizer.json\")\n",
    "print(f\"Loaded tokenizer with {new_tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Verify the loaded tokenizer works the same\n",
    "check_text = \"Testing if the loaded tokenizer works correctly.\"\n",
    "original_tokens = tokenizer.encode(check_text)\n",
    "loaded_tokens = new_tokenizer.encode(check_text)\n",
    "\n",
    "print(f\"Original tokenizer: {len(original_tokens)} tokens\")\n",
    "print(f\"Loaded tokenizer: {len(loaded_tokens)} tokens\")\n",
    "print(f\"Tokens match: {original_tokens == loaded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Implementing GPT-style Regex Pre-tokenization\n",
    "\n",
    "For a more advanced implementation, we can add regex pre-tokenization similar to GPT tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "    \"\"\"Enhanced tokenizer with regex-based pre-tokenization like GPT models.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # GPT-style regex pattern for splitting text before BPE\n",
    "        self.pattern = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text with regex pre-tokenization before applying BPE.\"\"\"\n",
    "        # First split text using regex pattern\n",
    "        segments = [m.group() for m in re.finditer(self.pattern, text)]\n",
    "        \n",
    "        # Apply BPE encoding to each segment\n",
    "        all_tokens = []\n",
    "        for segment in segments:\n",
    "            # Use the parent class's encoding logic for each segment\n",
    "            segment_tokens = super().encode(segment)\n",
    "            all_tokens.extend(segment_tokens)\n",
    "            \n",
    "        return all_tokens\n",
    "    \n",
    "    def save(self, file_path: str) -> None:\n",
    "        \"\"\"Save with additional regex pattern information.\"\"\"\n",
    "        # Get base data\n",
    "        super().save(file_path)\n",
    "        \n",
    "        # Add regex pattern to the saved data\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        data['pattern'] = self.pattern.pattern\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, file_path: str) -> None:\n",
    "        \"\"\"Load with regex pattern information.\"\"\"\n",
    "        super().load(file_path)\n",
    "        \n",
    "        # Load regex pattern if available\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 'pattern' in data:\n",
    "            self.pattern = re.compile(data['pattern'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer to vocab size 500\n",
      "Text size: 656 chars, 662 bytes\n",
      "Merge #0: pair (101, 32) -> 256, corpus now 647 tokens\n",
      "Merge #100: pair (355, 110) -> 356, corpus now 311 tokens\n",
      "Merge #200: pair (455, 277) -> 456, corpus now 211 tokens\n",
      "Basic tokenizer: 51 tokens\n",
      "Regex tokenizer: 57 tokens\n",
      "\n",
      "Basic tokenization:\n",
      "Tokenized into 51 tokens:\n",
      "[I][t]['][s‚ê£][n][o][t‚ê£][j][u][s][t‚ê£][to][k][en][iz][a][ti][o][n][,‚ê£][it]['][s‚ê£][BPE][‚ê£][to][k][en][iz][a][ti][o][n‚ê£][with][‚ê£re][g][e][x][‚ê£][p][re][-][p][r][o][c][e][s][s][ing][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 257 = 's‚ê£'\n",
      "Token 5: ID 110 = 'n'\n",
      "Token 6: ID 111 = 'o'\n",
      "Token 7: ID 260 = 't‚ê£'\n",
      "Token 8: ID 106 = 'j'\n",
      "Token 9: ID 117 = 'u'\n",
      "Token 10: ID 115 = 's'\n",
      "Token 11: ID 260 = 't‚ê£'\n",
      "Token 12: ID 305 = 'to'\n",
      "Token 13: ID 107 = 'k'\n",
      "Token 14: ID 269 = 'en'\n",
      "Token 15: ID 306 = 'iz'\n",
      "Token 16: ID 97 = 'a'\n",
      "Token 17: ID 266 = 'ti'\n",
      "Token 18: ID 111 = 'o'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 303 = ',‚ê£'\n",
      "Token 21: ID 346 = 'it'\n",
      "Token 22: ID 39 = '''\n",
      "Token 23: ID 257 = 's‚ê£'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '‚ê£'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 293 = 'n‚ê£'\n",
      "Token 34: ID 324 = 'with'\n",
      "Token 35: ID 312 = '‚ê£re'\n",
      "Token 36: ID 103 = 'g'\n",
      "Token 37: ID 101 = 'e'\n",
      "Token 38: ID 120 = 'x'\n",
      "Token 39: ID 32 = '‚ê£'\n",
      "Token 40: ID 112 = 'p'\n",
      "Token 41: ID 265 = 're'\n",
      "Token 42: ID 45 = '-'\n",
      "Token 43: ID 112 = 'p'\n",
      "Token 44: ID 114 = 'r'\n",
      "Token 45: ID 111 = 'o'\n",
      "Token 46: ID 99 = 'c'\n",
      "Token 47: ID 101 = 'e'\n",
      "Token 48: ID 115 = 's'\n",
      "Token 49: ID 115 = 's'\n",
      "Token 50: ID 264 = 'ing'\n",
      "Token 51: ID 33 = '!'\n",
      "\n",
      "Regex tokenization:\n",
      "Tokenized into 57 tokens:\n",
      "[I][t]['][s][‚ê£][n][o][t][‚ê£][j][u][s][t][‚ê£][to][k][en][iz][a][ti][o][n][,][‚ê£][it]['][s][‚ê£][BPE][‚ê£][to][k][en][iz][a][ti][o][n][‚ê£][with][‚ê£re][g][e][x][‚ê£][p][re][-][p][r][o][c][e][s][s][ing][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 115 = 's'\n",
      "Token 5: ID 32 = '‚ê£'\n",
      "Token 6: ID 110 = 'n'\n",
      "Token 7: ID 111 = 'o'\n",
      "Token 8: ID 116 = 't'\n",
      "Token 9: ID 32 = '‚ê£'\n",
      "Token 10: ID 106 = 'j'\n",
      "Token 11: ID 117 = 'u'\n",
      "Token 12: ID 115 = 's'\n",
      "Token 13: ID 116 = 't'\n",
      "Token 14: ID 32 = '‚ê£'\n",
      "Token 15: ID 305 = 'to'\n",
      "Token 16: ID 107 = 'k'\n",
      "Token 17: ID 269 = 'en'\n",
      "Token 18: ID 306 = 'iz'\n",
      "Token 19: ID 97 = 'a'\n",
      "Token 20: ID 266 = 'ti'\n",
      "Token 21: ID 111 = 'o'\n",
      "Token 22: ID 110 = 'n'\n",
      "Token 23: ID 44 = ','\n",
      "Token 24: ID 32 = '‚ê£'\n",
      "Token 25: ID 346 = 'it'\n",
      "Token 26: ID 39 = '''\n",
      "Token 27: ID 115 = 's'\n",
      "Token 28: ID 32 = '‚ê£'\n",
      "Token 29: ID 292 = 'BPE'\n",
      "Token 30: ID 32 = '‚ê£'\n",
      "Token 31: ID 305 = 'to'\n",
      "Token 32: ID 107 = 'k'\n",
      "Token 33: ID 269 = 'en'\n",
      "Token 34: ID 306 = 'iz'\n",
      "Token 35: ID 97 = 'a'\n",
      "Token 36: ID 266 = 'ti'\n",
      "Token 37: ID 111 = 'o'\n",
      "Token 38: ID 110 = 'n'\n",
      "Token 39: ID 32 = '‚ê£'\n",
      "Token 40: ID 324 = 'with'\n",
      "Token 41: ID 312 = '‚ê£re'\n",
      "Token 42: ID 103 = 'g'\n",
      "Token 43: ID 101 = 'e'\n",
      "Token 44: ID 120 = 'x'\n",
      "Token 45: ID 32 = '‚ê£'\n",
      "Token 46: ID 112 = 'p'\n",
      "Token 47: ID 265 = 're'\n",
      "Token 48: ID 45 = '-'\n",
      "Token 49: ID 112 = 'p'\n",
      "Token 50: ID 114 = 'r'\n",
      "Token 51: ID 111 = 'o'\n",
      "Token 52: ID 99 = 'c'\n",
      "Token 53: ID 101 = 'e'\n",
      "Token 54: ID 115 = 's'\n",
      "Token 55: ID 115 = 's'\n",
      "Token 56: ID 264 = 'ing'\n",
      "Token 57: ID 33 = '!'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '‚ê£',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " '‚ê£',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " '‚ê£',\n",
       " 'to',\n",
       " 'k',\n",
       " 'en',\n",
       " 'iz',\n",
       " 'a',\n",
       " 'ti',\n",
       " 'o',\n",
       " 'n',\n",
       " ',',\n",
       " '‚ê£',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " '‚ê£',\n",
       " 'BPE',\n",
       " '‚ê£',\n",
       " 'to',\n",
       " 'k',\n",
       " 'en',\n",
       " 'iz',\n",
       " 'a',\n",
       " 'ti',\n",
       " 'o',\n",
       " 'n',\n",
       " '‚ê£',\n",
       " 'with',\n",
       " '‚ê£re',\n",
       " 'g',\n",
       " 'e',\n",
       " 'x',\n",
       " '‚ê£',\n",
       " 'p',\n",
       " 're',\n",
       " '-',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'ing',\n",
       " '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the regex tokenizer\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.train(training_text, vocab_size=500, verbose=True)\n",
    "\n",
    "# Compare with the basic tokenizer\n",
    "compare_text = \"It's not just tokenization, it's BPE tokenization with regex pre-processing!\"\n",
    "\n",
    "basic_tokens = tokenizer.encode(compare_text)\n",
    "regex_tokens = regex_tokenizer.encode(compare_text)\n",
    "\n",
    "print(f\"Basic tokenizer: {len(basic_tokens)} tokens\")\n",
    "print(f\"Regex tokenizer: {len(regex_tokens)} tokens\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(\"\\nBasic tokenization:\")\n",
    "visualize_tokenization(tokenizer, compare_text)\n",
    "\n",
    "print(\"\\nRegex tokenization:\")\n",
    "visualize_tokenization(regex_tokenizer, compare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated a faithful implementation of Karpathy's minbpe approach, featuring:\n",
    "\n",
    "1. A minimal, byte-level BPE tokenizer\n",
    "2. Clean, educational code structure\n",
    "3. Complete training, encoding, and decoding functionality\n",
    "4. Visualization tools for understanding the tokenization process\n",
    "5. An advanced regex-based tokenizer extension\n",
    "\n",
    "The implementation preserves the essential characteristics of Karpathy's original: simplicity, readability, and educational value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BPE Tokenizer",
   "language": "python",
   "name": "bpe_tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
