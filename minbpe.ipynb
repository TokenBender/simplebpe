{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minbpe: Minimal Byte Pair Encoding Tokenizer\n",
    "\n",
    "This notebook contains a faithful implementation of Andrej Karpathy's minbpe tokenizer, emphasizing:\n",
    "- Clean, minimal code\n",
    "- Byte-level tokenization\n",
    "- Educational clarity\n",
    "\n",
    "The implementation follows the core design philosophy of Karpathy's approach, preserving the simplicity and readability of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set, Any, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Tokenizer Implementation\n",
    "\n",
    "We start with a basic tokenizer class that implements the core BPE algorithm without any regex-based preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"A minimal Byte Pair Encoding tokenizer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize with the base 256 tokens (raw bytes 0-255)\n",
    "        self.merges = {}  # (token1, token2) -> new_token_id \n",
    "        self.vocab = {}   # token_id -> token (bytes)\n",
    "        self.vocab_size = 0\n",
    "        self.special_tokens = {}\n",
    "        \n",
    "        # Pre-populate the vocabulary with the basic 256 byte tokens\n",
    "        for i in range(256):\n",
    "            token = bytes([i])\n",
    "            self.vocab[i] = token\n",
    "            \n",
    "        self.vocab_size = 256\n",
    "    \n",
    "    def train(self, text: str, vocab_size: int, verbose: bool = False) -> None:\n",
    "        \"\"\"Train the tokenizer on text, extending the vocabulary to the desired size.\"\"\"\n",
    "        # Convert text to bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        # Keep track of progress\n",
    "        if verbose:\n",
    "            print(f\"Training BPE tokenizer to vocab size {vocab_size}\")\n",
    "            print(f\"Text size: {len(text)} chars, {len(ids)} bytes\")\n",
    "        \n",
    "        # Iteratively merge the most frequent pair until we reach the desired vocab size\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            # Count frequencies of adjacent pairs\n",
    "            stats = self.get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "                \n",
    "            # Find the most frequent pair\n",
    "            pair = max(stats, key=stats.get)\n",
    "            \n",
    "            # Create a new token for this pair\n",
    "            token1, token2 = pair\n",
    "            new_token = self.vocab[token1] + self.vocab[token2]\n",
    "            new_id = self.vocab_size\n",
    "            \n",
    "            # Add merge to our vocabulary\n",
    "            self.merges[pair] = new_id\n",
    "            self.vocab[new_id] = new_token\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "            # Apply the merge to the current token list\n",
    "            ids = self.merge(ids, pair, new_id)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"Merge #{i}: pair {pair} -> {new_id}, corpus now {len(ids)} tokens\")\n",
    "    \n",
    "    def get_stats(self, ids: List[int]) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Count the frequencies of adjacent token pairs.\"\"\"\n",
    "        stats = Counter()\n",
    "        for i in range(len(ids) - 1):\n",
    "            pair = (ids[i], ids[i+1])\n",
    "            stats[pair] += 1\n",
    "        return stats\n",
    "    \n",
    "    def merge(self, ids: List[int], pair: Tuple[int, int], new_id: int) -> List[int]:\n",
    "        \"\"\"Replace all occurrences of a token pair with a new token ID.\"\"\"\n",
    "        # Create a new list for the merged result\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            # If we're at the last token, just add it\n",
    "            if i == len(ids) - 1:\n",
    "                new_ids.append(ids[i])\n",
    "                break\n",
    "            \n",
    "            # If current pair matches, merge and add the new token\n",
    "            if ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                new_ids.append(new_id)\n",
    "                i += 2  # Skip both tokens\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1  # Move to next token\n",
    "        \n",
    "        return new_ids\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        # Convert text to bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        # Apply merges iteratively, in the order they were learned\n",
    "        while len(ids) >= 2:\n",
    "            # Find valid merge pairs in the current sequence\n",
    "            pairs = [(ids[i], ids[i+1]) for i in range(len(ids)-1)]\n",
    "            valid_pairs = [(pair, self.merges[pair]) for pair in pairs if pair in self.merges]\n",
    "            \n",
    "            # If no valid pairs, we're done\n",
    "            if not valid_pairs:\n",
    "                break\n",
    "                \n",
    "            # Find the pair with the lowest merge ID (first learned)\n",
    "            pair, new_id = min(valid_pairs, key=lambda x: x[1])\n",
    "            \n",
    "            # Apply the merge\n",
    "            ids = self.merge(ids, pair, new_id)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        # Convert token IDs to bytes\n",
    "        bytes_list = []\n",
    "        for token_id in ids:\n",
    "            bytes_list.extend(self.vocab[token_id])\n",
    "        \n",
    "        # Convert bytes to UTF-8 text\n",
    "        text = bytes(bytes_list).decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def save(self, file_path: str) -> None:\n",
    "        \"\"\"Save the tokenizer to a file.\"\"\"\n",
    "        # Prepare model data - convert bytes to lists for JSON serialization\n",
    "        model_data = {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"merges\": {f\"{t1},{t2}\": idx for (t1, t2), idx in self.merges.items()},\n",
    "            \"vocab\": {str(i): list(t) for i, t in self.vocab.items() if i >= 256},\n",
    "            \"special_tokens\": self.special_tokens\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, file_path: str) -> None:\n",
    "        \"\"\"Load a tokenizer from a file.\"\"\"\n",
    "        # Read the model data\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        # Reset the tokenizer\n",
    "        self.__init__()\n",
    "        \n",
    "        # Load the vocabulary\n",
    "        self.vocab_size = model_data[\"vocab_size\"]\n",
    "        \n",
    "        # Add vocabulary items (skipping the base 256 bytes already initialized)\n",
    "        for token_id_str, token_bytes in model_data[\"vocab\"].items():\n",
    "            token_id = int(token_id_str)\n",
    "            self.vocab[token_id] = bytes(token_bytes)\n",
    "        \n",
    "        # Load merges\n",
    "        for pair_str, idx in model_data[\"merges\"].items():\n",
    "            t1, t2 = map(int, pair_str.split(\",\"))\n",
    "            self.merges[(t1, t2)] = idx\n",
    "        \n",
    "        # Load special tokens\n",
    "        self.special_tokens = model_data.get(\"special_tokens\", {})\n",
    "    \n",
    "    def token_to_str(self, token_id: int) -> str:\n",
    "        \"\"\"Get a string representation of a token for visualization.\"\"\"\n",
    "        token_bytes = self.vocab[token_id]\n",
    "        # Try to convert to UTF-8 string if possible\n",
    "        try:\n",
    "            s = token_bytes.decode('utf-8')\n",
    "            # Replace newlines, tabs, etc. for display\n",
    "            s = s.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            if len(s.strip()) == 0:\n",
    "                # If it's all whitespace, show hex\n",
    "                return f\"[hex: {token_bytes.hex()}]\"\n",
    "            return s\n",
    "        except UnicodeDecodeError:\n",
    "            # If not a valid UTF-8 string, show hex\n",
    "            return f\"[hex: {token_bytes.hex()}]\"\n",
    "    \n",
    "    def print_vocab(self, n=50) -> None:\n",
    "        \"\"\"Print the first n tokens in the vocabulary for inspection.\"\"\"\n",
    "        ids = sorted(self.vocab.keys())\n",
    "        skipped = max(0, len(ids) - n)\n",
    "        print(f\"Vocabulary size: {len(ids)} tokens\")\n",
    "        print(f\"Showing first {min(n, len(ids))} tokens:\")\n",
    "        for i, token_id in enumerate(ids[:n]):\n",
    "            s = self.token_to_str(token_id)\n",
    "            print(f\"Token {token_id}: {s}\")\n",
    "        if skipped > 0:\n",
    "            print(f\"... and {skipped} more tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tokenizer\n",
    "\n",
    "Let's test our implementation with a simple example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer to vocab size 500\n",
      "Text size: 656 chars, 662 bytes\n",
      "Merge #0: pair (101, 32) -> 256, corpus now 647 tokens\n",
      "Merge #100: pair (355, 110) -> 356, corpus now 311 tokens\n",
      "Merge #200: pair (455, 277) -> 456, corpus now 211 tokens\n",
      "Vocabulary size: 500 tokens\n",
      "Showing first 30 tokens:\n",
      "Token 0: \u0000\n",
      "Token 1: \u0001\n",
      "Token 2: \u0002\n",
      "Token 3: \u0003\n",
      "Token 4: \u0004\n",
      "Token 5: \u0005\n",
      "Token 6: \u0006\n",
      "Token 7: \u0007\n",
      "Token 8:\n",
      "Token 9: \\t\n",
      "Token 10: \\n\n",
      "Token 11: [hex: 0b]\n",
      "Token 12: [hex: 0c]\n",
      "Token 13: [hex: 0d]\n",
      "Token 14: \u000e\n",
      "Token 15: \u000f\n",
      "Token 16: \u0010\n",
      "Token 17: \u0011\n",
      "Token 18: \u0012\n",
      "Token 19: \u0013\n",
      "Token 20: \u0014\n",
      "Token 21: \u0015\n",
      "Token 22: \u0016\n",
      "Token 23: \u0017\n",
      "Token 24: \u0018\n",
      "Token 25: \u0019\n",
      "Token 26: \u001a\n",
      "Token 27: \u001b\n",
      "Token 28: [hex: 1c]\n",
      "Token 29: [hex: 1d]\n",
      "... and 470 more tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a simple training corpus\n",
    "training_text = \"\"\"\n",
    "Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of consecutive bytes in a sequence with a single, unused byte. In NLP, it is used as a subword tokenization algorithm.\n",
    "\n",
    "The BPE algorithm works as follows:\n",
    "1. Initialize the vocabulary with individual characters/bytes\n",
    "2. Count all pairs of adjacent symbols in the training corpus\n",
    "3. Merge the most frequent pair and add it to the vocabulary\n",
    "4. Repeat steps 2-3 until reaching the desired vocabulary size\n",
    "\n",
    "BPE can handle out-of-vocabulary words by splitting them into known subword units, making it effective for various languages and even emoji 👍🌍.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize our tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Train to a vocabulary size of 500\n",
    "tokenizer.train(training_text, vocab_size=500, verbose=True)\n",
    "\n",
    "# Show some of the learned tokens\n",
    "tokenizer.print_vocab(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "Now let's test encoding and decoding to verify the tokenizer works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded into 42 tokens: [292, 32, 305, 107, 269, 306, 97, 266, 111, 293, 282, 107, 257, 103, 265, 311, 102, 262, 32, 110, 97, 116, 117, 295, 108, 32, 267, 110, 103, 117, 97, 103, 256, 112, 114, 111, 99, 101, 115, 115, 264, 33]\n",
      "\n",
      "Token breakdown:\n",
      "Token 1: ID 292 = 'BPE'\n",
      "Token 2: ID 32 = '[hex: 20]'\n",
      "Token 3: ID 305 = 'to'\n",
      "Token 4: ID 107 = 'k'\n",
      "Token 5: ID 269 = 'en'\n",
      "Token 6: ID 306 = 'iz'\n",
      "Token 7: ID 97 = 'a'\n",
      "Token 8: ID 266 = 'ti'\n",
      "Token 9: ID 111 = 'o'\n",
      "Token 10: ID 293 = 'n '\n",
      "Token 11: ID 282 = 'wor'\n",
      "Token 12: ID 107 = 'k'\n",
      "Token 13: ID 257 = 's '\n",
      "Token 14: ID 103 = 'g'\n",
      "Token 15: ID 265 = 're'\n",
      "Token 16: ID 311 = 'at '\n",
      "Token 17: ID 102 = 'f'\n",
      "Token 18: ID 262 = 'or'\n",
      "Token 19: ID 32 = '[hex: 20]'\n",
      "Token 20: ID 110 = 'n'\n",
      "Token 21: ID 97 = 'a'\n",
      "Token 22: ID 116 = 't'\n",
      "Token 23: ID 117 = 'u'\n",
      "Token 24: ID 295 = 'ra'\n",
      "Token 25: ID 108 = 'l'\n",
      "Token 26: ID 32 = '[hex: 20]'\n",
      "Token 27: ID 267 = 'la'\n",
      "Token 28: ID 110 = 'n'\n",
      "Token 29: ID 103 = 'g'\n",
      "Token 30: ID 117 = 'u'\n",
      "Token 31: ID 97 = 'a'\n",
      "Token 32: ID 103 = 'g'\n",
      "Token 33: ID 256 = 'e '\n",
      "Token 34: ID 112 = 'p'\n",
      "Token 35: ID 114 = 'r'\n",
      "Token 36: ID 111 = 'o'\n",
      "Token 37: ID 99 = 'c'\n",
      "Token 38: ID 101 = 'e'\n",
      "Token 39: ID 115 = 's'\n",
      "Token 40: ID 115 = 's'\n",
      "Token 41: ID 264 = 'ing'\n",
      "Token 42: ID 33 = '!'\n",
      "\n",
      "Decoded text: 'BPE tokenization works great for natural language processing!'\n",
      "Round trip success: True\n"
     ]
    }
   ],
   "source": [
    "# Test with a new sentence\n",
    "test_text = \"BPE tokenization works great for natural language processing!\"\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"Encoded into {len(encoded)} tokens: {encoded}\")\n",
    "\n",
    "# Display each token\n",
    "print(\"\\nToken breakdown:\")\n",
    "for i, token_id in enumerate(encoded):\n",
    "    print(f\"Token {i+1}: ID {token_id} = '{tokenizer.token_to_str(token_id)}'\")\n",
    "\n",
    "# Decode the tokens back to text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nDecoded text: '{decoded}'\")\n",
    "print(f\"Round trip success: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Tokenization Efficiency\n",
    "\n",
    "Let's compute some metrics on the tokenization efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text            | Chars    | Tokens   | Ratio   | Encode (s) | Decode (s) | Success\n",
      "---------------------------------------------------------------------------\n",
      "English         | 44       | 35       | 1.26    | 0.0001     | 0.0000     | True\n",
      "Repeated        | 41       | 41       | 1.00    | 0.0000     | 0.0000     | True\n",
      "Numbers         | 32       | 32       | 1.00    | 0.0000     | 0.0000     | True\n",
      "Technical       | 60       | 50       | 1.20    | 0.0000     | 0.0000     | True\n",
      "Emoji           | 15       | 37       | 0.41    | 0.0000     | 0.0000     | True\n",
      "Mixed           | 35       | 48       | 0.73    | 0.0001     | 0.0000     | True\n"
     ]
    }
   ],
   "source": [
    "def measure_efficiency(tokenizer, texts):\n",
    "    \"\"\"Measure tokenization efficiency across multiple text samples.\"\"\"\n",
    "    results = []\n",
    "    for name, text in texts.items():\n",
    "        # Tokenize and measure\n",
    "        start_time = time.time()\n",
    "        tokens = tokenizer.encode(text)\n",
    "        encode_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        decode_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_count = len(text)\n",
    "        token_count = len(tokens)\n",
    "        compression_ratio = char_count / token_count\n",
    "        chars_per_second = char_count / encode_time if encode_time > 0 else 0\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"name\": name,\n",
    "            \"chars\": char_count,\n",
    "            \"tokens\": token_count,\n",
    "            \"ratio\": compression_ratio,\n",
    "            \"encode_time\": encode_time,\n",
    "            \"decode_time\": decode_time,\n",
    "            \"chars_per_second\": chars_per_second,\n",
    "            \"roundtrip_success\": text == decoded\n",
    "        })\n",
    "    \n",
    "    # Print results table\n",
    "    print(f\"{'Text':<15} | {'Chars':<8} | {'Tokens':<8} | {'Ratio':<7} | {'Encode (s)':<10} | {'Decode (s)':<10} | {'Success':<7}\")\n",
    "    print(\"-\" * 75)\n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<15} | {r['chars']:<8} | {r['tokens']:<8} | {r['ratio']:<7.2f} | {r['encode_time']:<10.4f} | {r['decode_time']:<10.4f} | {r['roundtrip_success']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test texts\n",
    "test_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Repeated\": \"hello hello hello hello hello hello hello\",\n",
    "    \"Numbers\": \"1234567890 1234567890 1234567890\",\n",
    "    \"Technical\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Emoji\": \"🙂 🌍 🚀 👨‍👩‍👧‍👦 🎉\",\n",
    "    \"Mixed\": \"Training at 3.5x speed: 😊 快速训练！速度提高\"\n",
    "}\n",
    "\n",
    "# Measure tokenization efficiency\n",
    "efficiency_results = measure_efficiency(tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Tokenization Process\n",
    "\n",
    "Let's create a visualization of how text gets split into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized into 34 tokens:\n",
      "[H][e][l][l][o][,␣][wor][l][d][!][␣][T][h][i][s␣][i][s␣a][␣][te][s][t␣][of][␣][BPE][␣][to][k][en][iz][a][ti][o][n][.]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 72 = 'H'\n",
      "Token 2: ID 101 = 'e'\n",
      "Token 3: ID 108 = 'l'\n",
      "Token 4: ID 108 = 'l'\n",
      "Token 5: ID 111 = 'o'\n",
      "Token 6: ID 303 = ',␣'\n",
      "Token 7: ID 282 = 'wor'\n",
      "Token 8: ID 108 = 'l'\n",
      "Token 9: ID 100 = 'd'\n",
      "Token 10: ID 33 = '!'\n",
      "Token 11: ID 32 = '␣'\n",
      "Token 12: ID 84 = 'T'\n",
      "Token 13: ID 104 = 'h'\n",
      "Token 14: ID 105 = 'i'\n",
      "Token 15: ID 257 = 's␣'\n",
      "Token 16: ID 105 = 'i'\n",
      "Token 17: ID 277 = 's␣a'\n",
      "Token 18: ID 32 = '␣'\n",
      "Token 19: ID 263 = 'te'\n",
      "Token 20: ID 115 = 's'\n",
      "Token 21: ID 260 = 't␣'\n",
      "Token 22: ID 300 = 'of'\n",
      "Token 23: ID 32 = '␣'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '␣'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 110 = 'n'\n",
      "Token 34: ID 46 = '.'\n"
     ]
    }
   ],
   "source": [
    "def visualize_tokenization(tokenizer, text):\n",
    "    \"\"\"Visualize how text is tokenized by showing token boundaries.\"\"\"\n",
    "    # Encode the text\n",
    "    ids = tokenizer.encode(text)\n",
    "    \n",
    "    # Get the bytes for each token\n",
    "    token_bytes = [tokenizer.vocab[id] for id in ids]\n",
    "    \n",
    "    # Try to display each token as text\n",
    "    visualized = []\n",
    "    for token in token_bytes:\n",
    "        try:\n",
    "            token_text = token.decode('utf-8')\n",
    "            # Replace whitespace for visibility\n",
    "            token_text = token_text.replace(' ', '␣').replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            visualized.append(token_text)\n",
    "        except UnicodeDecodeError:\n",
    "            # If not a valid UTF-8 sequence, show hex\n",
    "            visualized.append(f\"[{token.hex()}]\")\n",
    "    \n",
    "    # Display with token boundaries\n",
    "    print(f\"Tokenized into {len(ids)} tokens:\")\n",
    "    result = \"\"\n",
    "    for token in visualized:\n",
    "        result += f\"[{token}]\"\n",
    "    print(result)\n",
    "    \n",
    "    # Display each token with its ID\n",
    "    print(\"\\nDetailed token breakdown:\")\n",
    "    for i, (id, vis) in enumerate(zip(ids, visualized)):\n",
    "        print(f\"Token {i+1}: ID {id} = '{vis}'\")\n",
    "    \n",
    "    return visualized\n",
    "\n",
    "# Visualize tokenization for a sample text\n",
    "sample_text = \"Hello, world! This is a test of BPE tokenization.\"\n",
    "tokens_visualized = visualize_tokenization(tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Tokenizer\n",
    "\n",
    "Let's test the serialization and deserialization of our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer with 500 tokens\n",
      "Loaded tokenizer with 500 tokens\n",
      "Original tokenizer: 32 tokens\n",
      "Loaded tokenizer: 32 tokens\n",
      "Tokens match: True\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "print(f\"Saved tokenizer with {tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Create a new tokenizer and load the saved model\n",
    "new_tokenizer = Tokenizer()\n",
    "new_tokenizer.load(\"bpe_tokenizer.json\")\n",
    "print(f\"Loaded tokenizer with {new_tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Verify the loaded tokenizer works the same\n",
    "check_text = \"Testing if the loaded tokenizer works correctly.\"\n",
    "original_tokens = tokenizer.encode(check_text)\n",
    "loaded_tokens = new_tokenizer.encode(check_text)\n",
    "\n",
    "print(f\"Original tokenizer: {len(original_tokens)} tokens\")\n",
    "print(f\"Loaded tokenizer: {len(loaded_tokens)} tokens\")\n",
    "print(f\"Tokens match: {original_tokens == loaded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex-Based Tokenizer\n",
    "\n",
    "For more efficient tokenization in natural language processing, we can implement a regex-based pre-tokenization step before applying BPE merges. This helps the tokenizer better handle natural language boundaries like words and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "    \"\"\"Enhanced tokenizer with regex-based pre-tokenization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pat = re.compile(r'(\\s+|[a-zA-Z]+|[0-9]+|\\S)')\n",
    "        # Ensure all parent class attributes are present\n",
    "        self.merges = {}  # (token1, token2) -> new_token_id \n",
    "        self.vocab = {}   # token_id -> token (bytes)\n",
    "        self.token_to_id = {}  # token (bytes) -> token_id\n",
    "        self.vocab_size = 256\n",
    "        \n",
    "        # Pre-populate the vocabulary with the basic 256 byte tokens\n",
    "        for i in range(256):\n",
    "            token = bytes([i])\n",
    "            self.vocab[i] = token\n",
    "            self.token_to_id[token] = i\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Override encode to use regex-based pre-tokenization.\"\"\"\n",
    "        # First split using regex pattern\n",
    "        parts = [part.encode('utf-8') for part in re.findall(self.pat, text)]\n",
    "        \n",
    "        # Then encode each part with the base tokenizer\n",
    "        ids = []\n",
    "        for part in parts:\n",
    "            # Convert to bytes and start with raw byte tokens\n",
    "            bytes_list = list(part)\n",
    "            tokens = [bytes([b]) for b in bytes_list]\n",
    "            \n",
    "            # Apply merges iteratively, as in the base class\n",
    "            while len(tokens) >= 2:\n",
    "                # Find valid merge pairs in the current sequence\n",
    "                pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "                valid_pairs = [(pair, self.merges[pair]) for pair in pairs if pair in self.merges]\n",
    "                \n",
    "                # If no valid pairs, we're done\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Find the pair with the lowest merge ID (first learned)\n",
    "                pair, new_id = min(valid_pairs, key=lambda x: x[1])\n",
    "                \n",
    "                # Apply the merge\n",
    "                tokens = self.merge(tokens, pair, new_id)\n",
    "            \n",
    "            # Map tokens to IDs\n",
    "            ids.extend([self.token_to_id[token] for token in tokens])\n",
    "            \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens Support\n",
    "\n",
    "Let's enhance our tokenizer to support special tokens, similar to GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialTokensTokenizer(Tokenizer):\n",
    "    \"\"\"Enhanced tokenizer with support for special tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, special_tokens=None):\n",
    "        super().__init__()\n",
    "        # Initialize special tokens dict\n",
    "        self.special_tokens_dict = {}  # str -> id\n",
    "        self.special_tokens_inv = {}   # id -> str\n",
    "        \n",
    "        # Add special tokens if provided\n",
    "        if special_tokens:\n",
    "            self.add_special_tokens(special_tokens)\n",
    "    \n",
    "    def add_special_tokens(self, tokens_dict):\n",
    "        \"\"\"\n",
    "        Add special tokens to the tokenizer.\n",
    "        Args:\n",
    "            tokens_dict: Dictionary mapping token strings to their desired IDs\n",
    "                         e.g., {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        for token, idx in tokens_dict.items():\n",
    "            # Make sure ID doesn't conflict with existing vocab\n",
    "            if idx in self.vocab:\n",
    "                raise ValueError(f\"ID {idx} already exists in vocabulary\")\n",
    "            \n",
    "            # Add to vocab and special tokens dictionaries\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            self.vocab[idx] = token_bytes\n",
    "            self.special_tokens_dict[token] = idx\n",
    "            self.special_tokens_inv[idx] = token\n",
    "            \n",
    "            # Update vocab size if necessary\n",
    "            self.vocab_size = max(self.vocab_size, idx + 1)\n",
    "    \n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode text with special token handling.\n",
    "        \n",
    "        Args:\n",
    "            text: The text to encode\n",
    "            allowed_special: Set of special tokens to recognize, or \"all\" for all tokens\n",
    "        \"\"\"\n",
    "        # Handle allowed_special parameter\n",
    "        if allowed_special == \"all\":\n",
    "            allowed_special = set(self.special_tokens_dict.keys())\n",
    "        elif allowed_special is None:\n",
    "            allowed_special = set()  # No special tokens allowed\n",
    "        \n",
    "        # First check for special tokens if any are allowed\n",
    "        if allowed_special:\n",
    "            # This is a simple greedy approach - in production you might use regex\n",
    "            tokens = []\n",
    "            i = 0\n",
    "            while i < len(text):\n",
    "                # Check if any special token starts at this position\n",
    "                matched = False\n",
    "                for special in allowed_special:\n",
    "                    if text[i:].startswith(special):\n",
    "                        # Found a special token, add its ID\n",
    "                        tokens.append(self.special_tokens_dict[special])\n",
    "                        i += len(special)\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                if not matched:\n",
    "                    # No special token matched, process normally\n",
    "                    # Find the longest non-special token sequence\n",
    "                    j = i\n",
    "                    while j < len(text) and not any(text[j:].startswith(s) for s in allowed_special):\n",
    "                        j += 1\n",
    "                    \n",
    "                    # Encode this chunk normally\n",
    "                    if j > i:\n",
    "                        chunk_ids = super().encode(text[i:j])\n",
    "                        tokens.extend(chunk_ids)\n",
    "                        i = j\n",
    "            \n",
    "            return tokens\n",
    "        else:\n",
    "            # No special tokens, encode normally\n",
    "            return super().encode(text)\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        \"\"\"Save with special tokens information.\"\"\"\n",
    "        # Start with base functionality\n",
    "        super().save(file_path)\n",
    "        \n",
    "        # Add special tokens to the saved data\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Add special tokens dict\n",
    "        data['special_tokens_dict'] = self.special_tokens_dict\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, file_path):\n",
    "        \"\"\"Load with special tokens information.\"\"\"\n",
    "        super().load(file_path)\n",
    "        \n",
    "        # Load special tokens if available\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 'special_tokens_dict' in data:\n",
    "            self.special_tokens_dict = data['special_tokens_dict']\n",
    "            self.special_tokens_inv = {v: k for k, v in self.special_tokens_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 'Here is some text <|endoftext|> followed by a special token.'\n",
      "Encoded with 33 tokens: [72, 101, 114, 256, 105, 257, 115, 111, 109, 256, 263, 120, 260, 100257, 32, 102, 339, 108, 340, 101, 271, 279, 323, 112, 101, 99, 105, 307, 32, 305, 107, 269, 46]\n",
      "Decoded: 'Here is some text <|endoftext|> followed by a special token.'\n",
      "Round trip success: True\n",
      "\n",
      "Token visualization:\n",
      "[H][e][r][e ][i][s ][s][o][m][e ][te][x][t ][*<|endoftext|>*][ ][f][ol][l][ow][e][d ][by][ a s][p][e][c][i][al][ ][to][k][en][.]\n"
     ]
    }
   ],
   "source": [
    "# Test special tokens support\n",
    "# Create common GPT-style special tokens\n",
    "special_tokens = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100261\n",
    "}\n",
    "\n",
    "# Create and train a tokenizer with special tokens\n",
    "special_tokenizer = SpecialTokensTokenizer()\n",
    "special_tokenizer.train(training_text, vocab_size=500, verbose=False)\n",
    "special_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Test encoding/decoding with special tokens\n",
    "test_with_special = \"Here is some text <|endoftext|> followed by a special token.\"\n",
    "encoded_special = special_tokenizer.encode(test_with_special, allowed_special=\"all\")\n",
    "decoded_special = special_tokenizer.decode(encoded_special)\n",
    "\n",
    "print(f\"Original text: '{test_with_special}'\")\n",
    "print(f\"Encoded with {len(encoded_special)} tokens: {encoded_special}\")\n",
    "print(f\"Decoded: '{decoded_special}'\")\n",
    "print(f\"Round trip success: {test_with_special == decoded_special}\")\n",
    "\n",
    "# Visualize the tokens including the special token\n",
    "print(\"\\nToken visualization:\")\n",
    "token_strs = []\n",
    "for token_id in encoded_special:\n",
    "    if token_id in special_tokenizer.special_tokens_inv:\n",
    "        # This is a special token\n",
    "        token_strs.append(special_tokenizer.special_tokens_inv[token_id])\n",
    "    else:\n",
    "        # Regular token\n",
    "        token_bytes = special_tokenizer.vocab[token_id]\n",
    "        try:\n",
    "            token_str = token_bytes.decode('utf-8')\n",
    "            token_strs.append(token_str)\n",
    "        except UnicodeDecodeError:\n",
    "            token_strs.append(f\"[hex: {token_bytes.hex()}]\")\n",
    "\n",
    "# Print with token boundaries\n",
    "result = \"\"\n",
    "for token in token_strs:\n",
    "    if token in special_tokens:\n",
    "        # Highlight special tokens\n",
    "        result += f\"[*{token}*]\"\n",
    "    else:\n",
    "        result += f\"[{token}]\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Against Reference Tokenizers\n",
    "\n",
    "Let's compare our tokenizer implementation with other popular tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex tokenizer trained with vocabulary size: 256\n"
     ]
    }
   ],
   "source": [
    "# Initialize the regex tokenizer\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.train(training_text, vocab_size=500, verbose=False)\n",
    "print(\"Regex tokenizer trained with vocabulary size:\", len(regex_tokenizer.token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer       | Text       | Tokens   | Time (ms) \n",
      "--------------------------------------------------\n",
      "Basic BPE       | English    | 35       | 0.05      \n",
      "Basic BPE       | Code       | 50       | 0.04      \n",
      "Basic BPE       | Mixed      | 34       | 0.04      \n",
      "Basic BPE       | Repeated   | 23       | 0.01      \n",
      "Regex BPE       | English    | 44       | 0.25      \n",
      "Regex BPE       | Code       | 60       | 0.07      \n",
      "Regex BPE       | Mixed      | 43       | 0.04      \n",
      "Regex BPE       | Repeated   | 35       | 0.02      \n",
      "Special BPE     | English    | 35       | 0.09      \n",
      "Special BPE     | Code       | 50       | 0.08      \n",
      "Special BPE     | Mixed      | 34       | 0.06      \n",
      "Special BPE     | Repeated   | 23       | 0.03      \n"
     ]
    }
   ],
   "source": [
    "# This cell would normally import and compare with reference tokenizers\n",
    "# We provide pseudocode here as an example of what you would do\n",
    "\n",
    "\"\"\"\n",
    "# You would usually import tiktoken or transformers:\n",
    "# import tiktoken\n",
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# Define benchmark texts and functions\n",
    "benchmark_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Mixed\": \"Training at 3.5x speed 😊 быстрое обучение\",\n",
    "    \"Repeated\": \"token token token token token token\"\n",
    "}\n",
    "\n",
    "def benchmark_tokenizer(name, tokenizer_func, texts):\n",
    "    results = []\n",
    "    for text_name, text in texts.items():\n",
    "        # Measure encoding time\n",
    "        start_time = time.time()\n",
    "        tokens = tokenizer_func(text)\n",
    "        encode_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            \"tokenizer\": name,\n",
    "            \"text\": text_name,\n",
    "            \"tokens\": len(tokens),\n",
    "            \"encode_time_ms\": encode_time * 1000\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Define encoding functions for different tokenizers\n",
    "def encode_with_our_tokenizer(text):\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "# def encode_with_tiktoken(text):\n",
    "#     enc = tiktoken.get_encoding(\"gpt2\")\n",
    "#     return enc.encode(text)\n",
    "    \n",
    "# def encode_with_hf_tokenizer(text):\n",
    "#     hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#     return hf_tokenizer.encode(text)\n",
    "\n",
    "# Run benchmarks\n",
    "our_results = benchmark_tokenizer(\"Our BPE\", encode_with_our_tokenizer, benchmark_texts)\n",
    "# tiktoken_results = benchmark_tokenizer(\"tiktoken\", encode_with_tiktoken, benchmark_texts)\n",
    "# hf_results = benchmark_tokenizer(\"HuggingFace\", encode_with_hf_tokenizer, benchmark_texts)\n",
    "\n",
    "# all_results = our_results + tiktoken_results + hf_results\n",
    "all_results = our_results\n",
    "\n",
    "# Display results in a table\n",
    "print(f\"{'Tokenizer':<15} | {'Text':<10} | {'Tokens':<8} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in all_results:\n",
    "    print(f\"{r['tokenizer']:<15} | {r['text']:<10} | {r['tokens']:<8} | {r['encode_time_ms']:<10.2f}\")\n",
    "\n",
    "# Here you would normally create visualization comparing the tokenizers\n",
    "\"\"\"\n",
    "\n",
    "# For now, we'll just run a simple benchmark on our own tokenizer\n",
    "benchmark_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Mixed\": \"Training at 3.5x speed 😊 快速训练！\",\n",
    "    \"Repeated\": \"token token token token token token\"\n",
    "}\n",
    "\n",
    "def benchmark_our_tokenizers(texts):\n",
    "    results = []\n",
    "    tokenizers = {\n",
    "        \"Basic BPE\": tokenizer,\n",
    "        \"Regex BPE\": regex_tokenizer,\n",
    "        \"Special BPE\": special_tokenizer\n",
    "    }\n",
    "    \n",
    "    for name, tkn in tokenizers.items():\n",
    "        for text_name, text in texts.items():\n",
    "            # Measure encoding time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For special tokenizer, specify allowed_special\n",
    "            if name == \"Special BPE\":\n",
    "                tokens = tkn.encode(text, allowed_special=\"all\")\n",
    "            else:\n",
    "                tokens = tkn.encode(text)\n",
    "                \n",
    "            encode_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"tokenizer\": name,\n",
    "                \"text\": text_name,\n",
    "                \"tokens\": len(tokens),\n",
    "                \"encode_time_ms\": encode_time * 1000\n",
    "            })\n",
    "    return results\n",
    "\n",
    "results = benchmark_our_tokenizers(benchmark_texts)\n",
    "\n",
    "# Display results in a table\n",
    "print(f\"{'Tokenizer':<15} | {'Text':<10} | {'Tokens':<8} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"{r['tokenizer']:<15} | {r['text']:<10} | {r['tokens']:<8} | {r['encode_time_ms']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer       | Text       | Tokens   | Time (ms) \n",
      "--------------------------------------------------\n",
      "Basic BPE       | English    | 35       | 0.05      \n",
      "Basic BPE       | Code       | 50       | 0.04      \n",
      "Basic BPE       | Mixed      | 34       | 0.04      \n",
      "Basic BPE       | Repeated   | 23       | 0.01      \n",
      "Regex BPE       | English    | 44       | 0.07      \n",
      "Regex BPE       | Code       | 60       | 0.15      \n",
      "Regex BPE       | Mixed      | 43       | 0.05      \n",
      "Regex BPE       | Repeated   | 35       | 0.02      \n",
      "Special BPE     | English    | 35       | 0.08      \n",
      "Special BPE     | Code       | 50       | 0.09      \n",
      "Special BPE     | Mixed      | 34       | 0.06      \n",
      "Special BPE     | Repeated   | 23       | 0.03      \n"
     ]
    }
   ],
   "source": [
    "# Train the regex tokenizer first\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.train(training_text, vocab_size=500, verbose=False)\n",
    "\n",
    "# Now we can do benchmarking\n",
    "benchmark_texts = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Code\": \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    \"Mixed\": \"Training at 3.5x speed 😊 快速训练！\",\n",
    "    \"Repeated\": \"token token token token token token\"\n",
    "}\n",
    "\n",
    "def benchmark_our_tokenizers(texts):\n",
    "    results = []\n",
    "    tokenizers = {\n",
    "        \"Basic BPE\": tokenizer,\n",
    "        \"Regex BPE\": regex_tokenizer,\n",
    "        \"Special BPE\": special_tokenizer\n",
    "    }\n",
    "    \n",
    "    for name, tkn in tokenizers.items():\n",
    "        for text_name, text in texts.items():\n",
    "            # Measure encoding time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For special tokenizer, specify allowed_special\n",
    "            if name == \"Special BPE\":\n",
    "                tokens = tkn.encode(text, allowed_special=\"all\")\n",
    "            else:\n",
    "                tokens = tkn.encode(text)\n",
    "                \n",
    "            encode_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"tokenizer\": name,\n",
    "                \"text\": text_name,\n",
    "                \"tokens\": len(tokens),\n",
    "                \"encode_time_ms\": encode_time * 1000\n",
    "            })\n",
    "    return results\n",
    "\n",
    "results = benchmark_our_tokenizers(benchmark_texts)\n",
    "\n",
    "# Display results in a table\n",
    "print(f\"{'Tokenizer':<15} | {'Text':<10} | {'Tokens':<8} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"{r['tokenizer']:<15} | {r['text']:<10} | {r['tokens']:<8} | {r['encode_time_ms']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Token Visualization\n",
    "\n",
    "Let's improve our token visualization to better illustrate token boundaries and include additional token information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Regex Tokenizer\n",
    "\n",
    "Let's compare the basic tokenizer with our regex-based tokenizer to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tokenizer: 51 tokens\n",
      "Regex tokenizer: 76 tokens\n",
      "\n",
      "Basic tokenization:\n",
      "Tokenized into 51 tokens:\n",
      "[I][t]['][s␣][n][o][t␣][j][u][s][t␣][to][k][en][iz][a][ti][o][n][,␣][it]['][s␣][BPE][␣][to][k][en][iz][a][ti][o][n␣][with][␣re][g][e][x][␣][p][re][-][p][r][o][c][e][s][s][ing][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 257 = 's␣'\n",
      "Token 5: ID 110 = 'n'\n",
      "Token 6: ID 111 = 'o'\n",
      "Token 7: ID 260 = 't␣'\n",
      "Token 8: ID 106 = 'j'\n",
      "Token 9: ID 117 = 'u'\n",
      "Token 10: ID 115 = 's'\n",
      "Token 11: ID 260 = 't␣'\n",
      "Token 12: ID 305 = 'to'\n",
      "Token 13: ID 107 = 'k'\n",
      "Token 14: ID 269 = 'en'\n",
      "Token 15: ID 306 = 'iz'\n",
      "Token 16: ID 97 = 'a'\n",
      "Token 17: ID 266 = 'ti'\n",
      "Token 18: ID 111 = 'o'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 303 = ',␣'\n",
      "Token 21: ID 346 = 'it'\n",
      "Token 22: ID 39 = '''\n",
      "Token 23: ID 257 = 's␣'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '␣'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 293 = 'n␣'\n",
      "Token 34: ID 324 = 'with'\n",
      "Token 35: ID 312 = '␣re'\n",
      "Token 36: ID 103 = 'g'\n",
      "Token 37: ID 101 = 'e'\n",
      "Token 38: ID 120 = 'x'\n",
      "Token 39: ID 32 = '␣'\n",
      "Token 40: ID 112 = 'p'\n",
      "Token 41: ID 265 = 're'\n",
      "Token 42: ID 45 = '-'\n",
      "Token 43: ID 112 = 'p'\n",
      "Token 44: ID 114 = 'r'\n",
      "Token 45: ID 111 = 'o'\n",
      "Token 46: ID 99 = 'c'\n",
      "Token 47: ID 101 = 'e'\n",
      "Token 48: ID 115 = 's'\n",
      "Token 49: ID 115 = 's'\n",
      "Token 50: ID 264 = 'ing'\n",
      "Token 51: ID 33 = '!'\n",
      "\n",
      "Regex tokenization:\n",
      "Tokenized into 76 tokens:\n",
      "[I][t]['][s][␣][n][o][t][␣][j][u][s][t][␣][t][o][k][e][n][i][z][a][t][i][o][n][,][␣][i][t]['][s][␣][B][P][E][␣][t][o][k][e][n][i][z][a][t][i][o][n][␣][w][i][t][h][␣][r][e][g][e][x][␣][p][r][e][-][p][r][o][c][e][s][s][i][n][g][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 115 = 's'\n",
      "Token 5: ID 32 = '␣'\n",
      "Token 6: ID 110 = 'n'\n",
      "Token 7: ID 111 = 'o'\n",
      "Token 8: ID 116 = 't'\n",
      "Token 9: ID 32 = '␣'\n",
      "Token 10: ID 106 = 'j'\n",
      "Token 11: ID 117 = 'u'\n",
      "Token 12: ID 115 = 's'\n",
      "Token 13: ID 116 = 't'\n",
      "Token 14: ID 32 = '␣'\n",
      "Token 15: ID 116 = 't'\n",
      "Token 16: ID 111 = 'o'\n",
      "Token 17: ID 107 = 'k'\n",
      "Token 18: ID 101 = 'e'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 105 = 'i'\n",
      "Token 21: ID 122 = 'z'\n",
      "Token 22: ID 97 = 'a'\n",
      "Token 23: ID 116 = 't'\n",
      "Token 24: ID 105 = 'i'\n",
      "Token 25: ID 111 = 'o'\n",
      "Token 26: ID 110 = 'n'\n",
      "Token 27: ID 44 = ','\n",
      "Token 28: ID 32 = '␣'\n",
      "Token 29: ID 105 = 'i'\n",
      "Token 30: ID 116 = 't'\n",
      "Token 31: ID 39 = '''\n",
      "Token 32: ID 115 = 's'\n",
      "Token 33: ID 32 = '␣'\n",
      "Token 34: ID 66 = 'B'\n",
      "Token 35: ID 80 = 'P'\n",
      "Token 36: ID 69 = 'E'\n",
      "Token 37: ID 32 = '␣'\n",
      "Token 38: ID 116 = 't'\n",
      "Token 39: ID 111 = 'o'\n",
      "Token 40: ID 107 = 'k'\n",
      "Token 41: ID 101 = 'e'\n",
      "Token 42: ID 110 = 'n'\n",
      "Token 43: ID 105 = 'i'\n",
      "Token 44: ID 122 = 'z'\n",
      "Token 45: ID 97 = 'a'\n",
      "Token 46: ID 116 = 't'\n",
      "Token 47: ID 105 = 'i'\n",
      "Token 48: ID 111 = 'o'\n",
      "Token 49: ID 110 = 'n'\n",
      "Token 50: ID 32 = '␣'\n",
      "Token 51: ID 119 = 'w'\n",
      "Token 52: ID 105 = 'i'\n",
      "Token 53: ID 116 = 't'\n",
      "Token 54: ID 104 = 'h'\n",
      "Token 55: ID 32 = '␣'\n",
      "Token 56: ID 114 = 'r'\n",
      "Token 57: ID 101 = 'e'\n",
      "Token 58: ID 103 = 'g'\n",
      "Token 59: ID 101 = 'e'\n",
      "Token 60: ID 120 = 'x'\n",
      "Token 61: ID 32 = '␣'\n",
      "Token 62: ID 112 = 'p'\n",
      "Token 63: ID 114 = 'r'\n",
      "Token 64: ID 101 = 'e'\n",
      "Token 65: ID 45 = '-'\n",
      "Token 66: ID 112 = 'p'\n",
      "Token 67: ID 114 = 'r'\n",
      "Token 68: ID 111 = 'o'\n",
      "Token 69: ID 99 = 'c'\n",
      "Token 70: ID 101 = 'e'\n",
      "Token 71: ID 115 = 's'\n",
      "Token 72: ID 115 = 's'\n",
      "Token 73: ID 105 = 'i'\n",
      "Token 74: ID 110 = 'n'\n",
      "Token 75: ID 103 = 'g'\n",
      "Token 76: ID 33 = '!'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " '␣',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ',',\n",
       " '␣',\n",
       " 'i',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'B',\n",
       " 'P',\n",
       " 'E',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " '␣',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '␣',\n",
       " 'r',\n",
       " 'e',\n",
       " 'g',\n",
       " 'e',\n",
       " 'x',\n",
       " '␣',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " '-',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with the basic tokenizer\n",
    "compare_text = \"It's not just tokenization, it's BPE tokenization with regex pre-processing!\"\n",
    "\n",
    "basic_tokens = tokenizer.encode(compare_text)\n",
    "regex_tokens = regex_tokenizer.encode(compare_text)\n",
    "\n",
    "print(f\"Basic tokenizer: {len(basic_tokens)} tokens\")\n",
    "print(f\"Regex tokenizer: {len(regex_tokens)} tokens\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(\"\\nBasic tokenization:\")\n",
    "visualize_tokenization(tokenizer, compare_text)\n",
    "\n",
    "print(\"\\nRegex tokenization:\")\n",
    "visualize_tokenization(regex_tokenizer, compare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer to vocab size 500\n",
      "Text size: 656 chars, 662 bytes\n",
      "Merge #0: pair (101, 32) -> 256, corpus now 647 tokens\n",
      "Merge #100: pair (355, 110) -> 356, corpus now 311 tokens\n",
      "Merge #200: pair (455, 277) -> 456, corpus now 211 tokens\n",
      "Basic tokenizer: 51 tokens\n",
      "Regex tokenizer: 76 tokens\n",
      "\n",
      "Basic tokenization:\n",
      "Tokenized into 51 tokens:\n",
      "[I][t]['][s␣][n][o][t␣][j][u][s][t␣][to][k][en][iz][a][ti][o][n][,␣][it]['][s␣][BPE][␣][to][k][en][iz][a][ti][o][n␣][with][␣re][g][e][x][␣][p][re][-][p][r][o][c][e][s][s][ing][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 257 = 's␣'\n",
      "Token 5: ID 110 = 'n'\n",
      "Token 6: ID 111 = 'o'\n",
      "Token 7: ID 260 = 't␣'\n",
      "Token 8: ID 106 = 'j'\n",
      "Token 9: ID 117 = 'u'\n",
      "Token 10: ID 115 = 's'\n",
      "Token 11: ID 260 = 't␣'\n",
      "Token 12: ID 305 = 'to'\n",
      "Token 13: ID 107 = 'k'\n",
      "Token 14: ID 269 = 'en'\n",
      "Token 15: ID 306 = 'iz'\n",
      "Token 16: ID 97 = 'a'\n",
      "Token 17: ID 266 = 'ti'\n",
      "Token 18: ID 111 = 'o'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 303 = ',␣'\n",
      "Token 21: ID 346 = 'it'\n",
      "Token 22: ID 39 = '''\n",
      "Token 23: ID 257 = 's␣'\n",
      "Token 24: ID 292 = 'BPE'\n",
      "Token 25: ID 32 = '␣'\n",
      "Token 26: ID 305 = 'to'\n",
      "Token 27: ID 107 = 'k'\n",
      "Token 28: ID 269 = 'en'\n",
      "Token 29: ID 306 = 'iz'\n",
      "Token 30: ID 97 = 'a'\n",
      "Token 31: ID 266 = 'ti'\n",
      "Token 32: ID 111 = 'o'\n",
      "Token 33: ID 293 = 'n␣'\n",
      "Token 34: ID 324 = 'with'\n",
      "Token 35: ID 312 = '␣re'\n",
      "Token 36: ID 103 = 'g'\n",
      "Token 37: ID 101 = 'e'\n",
      "Token 38: ID 120 = 'x'\n",
      "Token 39: ID 32 = '␣'\n",
      "Token 40: ID 112 = 'p'\n",
      "Token 41: ID 265 = 're'\n",
      "Token 42: ID 45 = '-'\n",
      "Token 43: ID 112 = 'p'\n",
      "Token 44: ID 114 = 'r'\n",
      "Token 45: ID 111 = 'o'\n",
      "Token 46: ID 99 = 'c'\n",
      "Token 47: ID 101 = 'e'\n",
      "Token 48: ID 115 = 's'\n",
      "Token 49: ID 115 = 's'\n",
      "Token 50: ID 264 = 'ing'\n",
      "Token 51: ID 33 = '!'\n",
      "\n",
      "Regex tokenization:\n",
      "Tokenized into 76 tokens:\n",
      "[I][t]['][s][␣][n][o][t][␣][j][u][s][t][␣][t][o][k][e][n][i][z][a][t][i][o][n][,][␣][i][t]['][s][␣][B][P][E][␣][t][o][k][e][n][i][z][a][t][i][o][n][␣][w][i][t][h][␣][r][e][g][e][x][␣][p][r][e][-][p][r][o][c][e][s][s][i][n][g][!]\n",
      "\n",
      "Detailed token breakdown:\n",
      "Token 1: ID 73 = 'I'\n",
      "Token 2: ID 116 = 't'\n",
      "Token 3: ID 39 = '''\n",
      "Token 4: ID 115 = 's'\n",
      "Token 5: ID 32 = '␣'\n",
      "Token 6: ID 110 = 'n'\n",
      "Token 7: ID 111 = 'o'\n",
      "Token 8: ID 116 = 't'\n",
      "Token 9: ID 32 = '␣'\n",
      "Token 10: ID 106 = 'j'\n",
      "Token 11: ID 117 = 'u'\n",
      "Token 12: ID 115 = 's'\n",
      "Token 13: ID 116 = 't'\n",
      "Token 14: ID 32 = '␣'\n",
      "Token 15: ID 116 = 't'\n",
      "Token 16: ID 111 = 'o'\n",
      "Token 17: ID 107 = 'k'\n",
      "Token 18: ID 101 = 'e'\n",
      "Token 19: ID 110 = 'n'\n",
      "Token 20: ID 105 = 'i'\n",
      "Token 21: ID 122 = 'z'\n",
      "Token 22: ID 97 = 'a'\n",
      "Token 23: ID 116 = 't'\n",
      "Token 24: ID 105 = 'i'\n",
      "Token 25: ID 111 = 'o'\n",
      "Token 26: ID 110 = 'n'\n",
      "Token 27: ID 44 = ','\n",
      "Token 28: ID 32 = '␣'\n",
      "Token 29: ID 105 = 'i'\n",
      "Token 30: ID 116 = 't'\n",
      "Token 31: ID 39 = '''\n",
      "Token 32: ID 115 = 's'\n",
      "Token 33: ID 32 = '␣'\n",
      "Token 34: ID 66 = 'B'\n",
      "Token 35: ID 80 = 'P'\n",
      "Token 36: ID 69 = 'E'\n",
      "Token 37: ID 32 = '␣'\n",
      "Token 38: ID 116 = 't'\n",
      "Token 39: ID 111 = 'o'\n",
      "Token 40: ID 107 = 'k'\n",
      "Token 41: ID 101 = 'e'\n",
      "Token 42: ID 110 = 'n'\n",
      "Token 43: ID 105 = 'i'\n",
      "Token 44: ID 122 = 'z'\n",
      "Token 45: ID 97 = 'a'\n",
      "Token 46: ID 116 = 't'\n",
      "Token 47: ID 105 = 'i'\n",
      "Token 48: ID 111 = 'o'\n",
      "Token 49: ID 110 = 'n'\n",
      "Token 50: ID 32 = '␣'\n",
      "Token 51: ID 119 = 'w'\n",
      "Token 52: ID 105 = 'i'\n",
      "Token 53: ID 116 = 't'\n",
      "Token 54: ID 104 = 'h'\n",
      "Token 55: ID 32 = '␣'\n",
      "Token 56: ID 114 = 'r'\n",
      "Token 57: ID 101 = 'e'\n",
      "Token 58: ID 103 = 'g'\n",
      "Token 59: ID 101 = 'e'\n",
      "Token 60: ID 120 = 'x'\n",
      "Token 61: ID 32 = '␣'\n",
      "Token 62: ID 112 = 'p'\n",
      "Token 63: ID 114 = 'r'\n",
      "Token 64: ID 101 = 'e'\n",
      "Token 65: ID 45 = '-'\n",
      "Token 66: ID 112 = 'p'\n",
      "Token 67: ID 114 = 'r'\n",
      "Token 68: ID 111 = 'o'\n",
      "Token 69: ID 99 = 'c'\n",
      "Token 70: ID 101 = 'e'\n",
      "Token 71: ID 115 = 's'\n",
      "Token 72: ID 115 = 's'\n",
      "Token 73: ID 105 = 'i'\n",
      "Token 74: ID 110 = 'n'\n",
      "Token 75: ID 103 = 'g'\n",
      "Token 76: ID 33 = '!'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " '␣',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ',',\n",
       " '␣',\n",
       " 'i',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " '␣',\n",
       " 'B',\n",
       " 'P',\n",
       " 'E',\n",
       " '␣',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " '␣',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '␣',\n",
       " 'r',\n",
       " 'e',\n",
       " 'g',\n",
       " 'e',\n",
       " 'x',\n",
       " '␣',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " '-',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the regex tokenizer\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "regex_tokenizer.train(training_text, vocab_size=500, verbose=True)\n",
    "\n",
    "# Compare with the basic tokenizer\n",
    "compare_text = \"It's not just tokenization, it's BPE tokenization with regex pre-processing!\"\n",
    "\n",
    "basic_tokens = tokenizer.encode(compare_text)\n",
    "regex_tokens = regex_tokenizer.encode(compare_text)\n",
    "\n",
    "print(f\"Basic tokenizer: {len(basic_tokens)} tokens\")\n",
    "print(f\"Regex tokenizer: {len(regex_tokens)} tokens\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(\"\\nBasic tokenization:\")\n",
    "visualize_tokenization(tokenizer, compare_text)\n",
    "\n",
    "print(\"\\nRegex tokenization:\")\n",
    "visualize_tokenization(regex_tokenizer, compare_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BPE Tokenizer",
   "language": "python",
   "name": "bpe_tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
